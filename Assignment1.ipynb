{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz13BvtmN49a"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WE9zjHoMn6h",
        "outputId": "6c849bdb-2940-4f61-ad93-1018a2e5c332"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in c:\\users\\morne\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.14.0)\n",
            "Requirement already satisfied: nltk in c:\\users\\morne\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.9.1)\n",
            "Requirement already satisfied: click in c:\\users\\morne\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in c:\\users\\morne\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\morne\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in c:\\users\\morne\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\morne\\appdata\\roaming\\python\\python39\\site-packages (from click->nltk) (0.4.6)\n",
            "Requirement already satisfied: tqdm in c:\\users\\morne\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.67.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\morne\\appdata\\roaming\\python\\python39\\site-packages (from tqdm) (0.4.6)\n",
            "Requirement already satisfied: simplejson in c:\\users\\morne\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.19.3)\n",
            "Requirement already satisfied: gensim in c:\\users\\morne\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\morne\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\morne\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\morne\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in c:\\users\\morne\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji\n",
        "!pip install nltk\n",
        "!pip install tqdm\n",
        "!pip install simplejson\n",
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "C8iazcwMNM8Q"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import shutil\n",
        "import json\n",
        "import urllib\n",
        "import tarfile\n",
        "from pathlib import Path\n",
        "import simplejson as sj\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from typing import Dict, OrderedDict, List\n",
        "\n",
        "import re\n",
        "import emoji\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "import gensim\n",
        "import gensim.downloader as gloader\n",
        "\n",
        "\n",
        "\n",
        "from typing import Iterable\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aorcZzjLOQII"
      },
      "source": [
        "# Task 1: Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7xVZIPFOXPT"
      },
      "source": [
        "## 1. Downloading the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz0BiOVBOhui"
      },
      "source": [
        "First of all we need to **download** the `A1/data` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "5CFbK72BOtVu"
      },
      "outputs": [],
      "source": [
        "class DownloadProgressBar(tqdm):\n",
        "    def update_to(self, b=1, bsize=1, tsize=None):\n",
        "        if tsize is not None:\n",
        "            self.total = tsize\n",
        "        self.update(b * bsize - self.n)\n",
        "\n",
        "def download_url(download_path: Path, url: str):\n",
        "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
        "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
        "        urllib.request.urlretrieve(url, filename=download_path, reporthook=t.update_to)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "4Jwr7Ns6Ot1i"
      },
      "outputs": [],
      "source": [
        "def download_dataset(download_path: Path, url: str):\n",
        "    print(\"Downloading dataset...\")\n",
        "    download_url(url=url, download_path=download_path)\n",
        "    print(\"Download complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "jsZv2RSOViC8"
      },
      "outputs": [],
      "source": [
        "# Here we put all the urls\n",
        "urls = {\n",
        "    \"training\": \"https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/main/2024-2025/Assignment%201/data/training.json\",\n",
        "    \"test\": \"https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/main/2024-2025/Assignment%201/data/test.json\",\n",
        "    \"validation\": \"https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/main/2024-2025/Assignment%201/data/validation.json\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NvOZ5g2O8Ra",
        "outputId": "f567f9be-9600-4a03-80dc-42124b162ad5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current work directory: c:\\Users\\morne\\Desktop\\ProjectNLP\\git_repository\\NLP_Assignments\n"
          ]
        }
      ],
      "source": [
        "print(f\"Current work directory: {Path.cwd()}\")\n",
        "dataset_folder = Path.cwd().joinpath(\"Datasets\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "mGQb4TxtRYYm"
      },
      "outputs": [],
      "source": [
        "if not dataset_folder.exists():\n",
        "    dataset_folder.mkdir(parents=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r99L1rTzV12C",
        "outputId": "bd1b277a-46e9-4fde-8c5e-91b0a675993d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "training.json: 6.23MB [00:01, 5.32MB/s]                            \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Download complete!\n",
            "Downloading dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "test.json: 500kB [00:00, 1.47MB/s]                            \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Download complete!\n",
            "Downloading dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "validation.json: 1.16MB [00:00, 2.51MB/s]                            "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Download complete!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for name, url in urls.items():\n",
        "    download_path = dataset_folder.joinpath(f\"{name}.json\")\n",
        "    download_dataset(download_path, url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97p0MGpIYdGY"
      },
      "source": [
        "## 2. Load the three JSON files and encode them as pandas dataframes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "tjPonkkIduMr"
      },
      "outputs": [],
      "source": [
        "def load_json_file(file_path: Path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "mUz9IDNUcb7J"
      },
      "outputs": [],
      "source": [
        "dataframe_rows = []\n",
        "\n",
        "\n",
        "for name, url in urls.items():\n",
        "    # per ogni file creiamo il file_path e leggiamo il file\n",
        "    file_path = dataset_folder.joinpath(f\"{name}.json\")\n",
        "\n",
        "    json_data = load_json_file(file_path)\n",
        "\n",
        "    # per ogni chiave nel json_data creo una dataframe_row\n",
        "    for key in json_data.keys():\n",
        "        df_row = json_data[key]\n",
        "        df_row[\"split\"] = name\n",
        "        dataframe_rows.append(df_row)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "u2xd6biKiKbW"
      },
      "outputs": [],
      "source": [
        "folder = Path.cwd().joinpath(\"Datasets\", \"Dataframes\")\n",
        "if not folder.exists():\n",
        "    folder.mkdir(parents=True)\n",
        "\n",
        "\n",
        "# transform the list of rows in a proper dataframe\n",
        "df = pd.DataFrame(dataframe_rows)\n",
        "\n",
        "for name, url in urls.items():\n",
        "  df_path = folder.with_name(name + \".pkl\")\n",
        "  df.to_pickle(df_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aegG34haxrJH"
      },
      "source": [
        "## 3. Generate hard labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6VZjWS-xvb5"
      },
      "source": [
        "Generate hard labels for Task 1 using majority voting and store them in a new dataframe column called `hard_label_task1`. Items without a clear majority will be removed from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "sORAVONvpU_v"
      },
      "outputs": [],
      "source": [
        "def compute_majority_voting(labels: list):\n",
        "\n",
        "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
        "    majority_label = np.argwhere(counts == np.max(counts))\n",
        "\n",
        "    majority_label = unique_labels[majority_label].flatten().tolist()\n",
        "\n",
        "    if len(majority_label) > 1:\n",
        "        majority_label = None\n",
        "\n",
        "\n",
        "    return majority_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "0LP9rRC1zG0Q"
      },
      "outputs": [],
      "source": [
        "def generate_hard_labels(df):\n",
        "    hard_labels = []\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        # Estrai le etichette dalla colonna 'labels_task1'\n",
        "        labels = row['labels_task1']\n",
        "        # print(labels)\n",
        "\n",
        "        # Verifica se 'labels' è una lista e contiene elementi\n",
        "        if isinstance(labels, list) and len(labels) > 0:\n",
        "            # Calcola la moda (voto di maggioranza)\n",
        "            most_common_label = compute_majority_voting(labels)\n",
        "            # print(most_common_label)\n",
        "            hard_labels.append(most_common_label)\n",
        "\n",
        "    # Aggiungi le hard labels come nuova colonna\n",
        "    df['hard_label_task1'] = hard_labels\n",
        "\n",
        "    # Rimuovi le righe senza una chiara maggioranza (se necessario)\n",
        "    df = df[df['hard_label_task1'].notnull()]\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckDgdopDz3d2",
        "outputId": "65dd6169-843c-435a-96a3-004c6b61be52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  id_EXIST lang                                              tweet  \\\n",
            "0   100001   es  @TheChiflis Ignora al otro, es un capullo.El p...   \n",
            "1   100002   es  @ultimonomada_ Si comicsgate se parece en algo...   \n",
            "2   100003   es  @Steven2897 Lee sobre Gamergate, y como eso ha...   \n",
            "4   100005   es  @novadragon21 @icep4ck @TvDannyZ Entonces como...   \n",
            "5   100006   es  @yonkykong Aaah sí. Andrew Dobson. El que se d...   \n",
            "\n",
            "   number_annotators                                         annotators  \\\n",
            "0                  6  [Annotator_1, Annotator_2, Annotator_3, Annota...   \n",
            "1                  6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
            "2                  6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
            "4                  6  [Annotator_19, Annotator_20, Annotator_21, Ann...   \n",
            "5                  6  [Annotator_25, Annotator_26, Annotator_27, Ann...   \n",
            "\n",
            "    gender_annotators                          age_annotators  \\\n",
            "0  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
            "1  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
            "2  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
            "4  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
            "5  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
            "\n",
            "                    labels_task1  \\\n",
            "0  [YES, YES, NO, YES, YES, YES]   \n",
            "1      [NO, NO, NO, NO, YES, NO]   \n",
            "2       [NO, NO, NO, NO, NO, NO]   \n",
            "4   [YES, NO, YES, NO, YES, YES]   \n",
            "5       [NO, NO, NO, NO, NO, NO]   \n",
            "\n",
            "                                        labels_task2  \\\n",
            "0  [REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...   \n",
            "1                            [-, -, -, -, DIRECT, -]   \n",
            "2                                 [-, -, -, -, -, -]   \n",
            "4  [REPORTED, -, JUDGEMENTAL, -, JUDGEMENTAL, DIR...   \n",
            "5                                 [-, -, -, -, -, -]   \n",
            "\n",
            "                                        labels_task3     split  \\\n",
            "0  [[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...  training   \n",
            "1       [[-], [-], [-], [-], [OBJECTIFICATION], [-]]  training   \n",
            "2                     [[-], [-], [-], [-], [-], [-]]  training   \n",
            "4  [[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [-...  training   \n",
            "5                     [[-], [-], [-], [-], [-], [-]]  training   \n",
            "\n",
            "  hard_label_task1  \n",
            "0            [YES]  \n",
            "1             [NO]  \n",
            "2             [NO]  \n",
            "4            [YES]  \n",
            "5             [NO]  \n"
          ]
        }
      ],
      "source": [
        "df = generate_hard_labels(df)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VdYgdgwqsNu"
      },
      "source": [
        "## 4. Filter the DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVfoZzusqy0I"
      },
      "source": [
        "Filter the DataFrame to keep only rows where the `lang` column is `'en'`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WYAWKilq1tl",
        "outputId": "ca75b8d0-78e1-495e-b38e-85aa8ae9871d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3314, 12)\n"
          ]
        }
      ],
      "source": [
        "df = df[df['lang'] == 'en']\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etQQV0YTrN9Q"
      },
      "source": [
        "## 5. Remove unwanted columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4nDBRNurTT6"
      },
      "source": [
        "Keep only `id_EXIST`, `lang`, `tweet`, and `hard_label_task1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "hb0-lUqPrZEH"
      },
      "outputs": [],
      "source": [
        "def remove_unwanted_columns(df):\n",
        "\n",
        "    columns_to_keep = ['id_EXIST', 'lang', 'tweet', 'hard_label_task1', 'split']\n",
        "    df = df[columns_to_keep]\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "YoZB0G8xrgcA"
      },
      "outputs": [],
      "source": [
        "df = remove_unwanted_columns(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68p9vTuLru-p"
      },
      "source": [
        "## 6. Encode the hard_label_task1 column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70CeoJ1hr-rB"
      },
      "source": [
        "Use 1 to represent \"YES\" and 0 to represent \"NO\" in the `hard_label_task1 column`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lh8WC291tNGA",
        "outputId": "2d609029-575a-40f3-b889-7eaee15c5cc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     id_EXIST lang                                              tweet  \\\n",
            "3661   200002   en  Writing a uni essay in my local pub with a cof...   \n",
            "3662   200003   en  @UniversalORL it is 2021 not 1921. I dont appr...   \n",
            "3665   200006   en  According to a customer I have plenty of time ...   \n",
            "3666   200007   en  So only 'blokes' drink beer? Sorry, but if you...   \n",
            "3667   200008   en  New to the shelves this week - looking forward...   \n",
            "\n",
            "      hard_label_task1     split  \n",
            "3661                 1  training  \n",
            "3662                 1  training  \n",
            "3665                 1  training  \n",
            "3666                 1  training  \n",
            "3667                 0  training  \n"
          ]
        }
      ],
      "source": [
        "df['hard_label_task1'] = df['hard_label_task1'].apply(lambda x: 1 if x[0] == 'YES' else 0)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyAK2RBGu8Bq"
      },
      "source": [
        "# Task 2: Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9rsrT-WJDoK",
        "outputId": "6611141a-42fb-435a-cb04-3917ba830cdd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\morne\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\morne\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\morne\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     C:\\Users\\morne\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\morne\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\morne\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3yM7nuc_DpZ"
      },
      "source": [
        "Check this link: [all existing emojis](https://www.unicode.org/Public/emoji/1.0//emoji-data.txt). And also this: [emojis unicode consortium](https://unicode.org/emoji/charts/full-emoji-list.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "GyDbd9BxU31A"
      },
      "outputs": [],
      "source": [
        "def lower(text):\n",
        "    return text.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "2U3Eg7_4_-Ic"
      },
      "outputs": [],
      "source": [
        "def remove_emoji(text):\n",
        "    return emoji.replace_emoji(text, replace='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "69qq6Fe25xMc"
      },
      "outputs": [],
      "source": [
        "def remove_hastag(text):\n",
        "    at = re.compile(r'#\\S+')\n",
        "    return at.sub(r'',text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "rp8HhWlh5NEu"
      },
      "outputs": [],
      "source": [
        "def remove_mention(text):\n",
        "    at = re.compile(r'@\\S+')\n",
        "    return at.sub(r'',text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "GwVvkIsa64wk"
      },
      "outputs": [],
      "source": [
        "def remove_URL(text):\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url.sub(r'',text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "iLRDqOqS7QKt"
      },
      "outputs": [],
      "source": [
        "def remove_special_characters(text):\n",
        "    pattern = r'[^a-zA-Z0-9\\s]'\n",
        "    return re.sub(pattern, '', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "v__lt8yu8Oap"
      },
      "outputs": [],
      "source": [
        "def remove_quotes(text):\n",
        "    pattern = r'^\"|\"$‘’'\n",
        "    return re.sub(pattern, '', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "6aRkZPTP9KG0"
      },
      "outputs": [],
      "source": [
        "def remove_extra_spaces(text):\n",
        "    text.strip()\n",
        "    pattern = r'\\s+'\n",
        "    return re.sub(pattern, ' ', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_word_repetitions(text):\n",
        "    return  re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "Eb2ni1lLI4oH"
      },
      "outputs": [],
      "source": [
        "def lemmatize(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "    def get_wordnet_key(pos_tag):\n",
        "        if pos_tag.startswith('J'):\n",
        "            return wordnet.ADJ\n",
        "        elif pos_tag.startswith('V'):\n",
        "            return wordnet.VERB\n",
        "        elif pos_tag.startswith('N'):\n",
        "            return wordnet.NOUN\n",
        "        elif pos_tag.startswith('R'):\n",
        "            return wordnet.ADV\n",
        "        else:\n",
        "            return 'n'\n",
        "\n",
        "\n",
        "    def lem_text(text: str):\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "        tagged = pos_tag(tokens)\n",
        "        words = [lemmatizer.lemmatize(word, get_wordnet_key(tag)) for word, tag in tagged]\n",
        "        return \" \".join(words)\n",
        "\n",
        "\n",
        "    return lem_text(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "Xid-F6PgLxoN"
      },
      "outputs": [],
      "source": [
        "def clean_tweet(tweet: str):\n",
        "    tweet = lower(tweet)\n",
        "    tweet = remove_emoji(tweet)\n",
        "    tweet = remove_hastag(tweet)\n",
        "    tweet = remove_mention(tweet)\n",
        "    tweet = remove_URL(tweet)\n",
        "    tweet = remove_special_characters(tweet)\n",
        "    tweet = remove_quotes(tweet)\n",
        "    tweet=remove_word_repetitions(tweet)\n",
        "    tweet = remove_extra_spaces(tweet)\n",
        "    tweet = lemmatize(tweet)\n",
        "    return tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "93acNCscMDRL"
      },
      "outputs": [],
      "source": [
        "df['tweet'] = df['tweet'].apply(clean_tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcwvF65MNAXd",
        "outputId": "1b1ba20f-e91e-43f6-e0da-dc6b9caffa1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3661    write a uni essay in my local pub with a coffe...\n",
            "3662    it be 2021 not 1921 i dont appreciate that on ...\n",
            "3665    accord to a customer i have plenty of time to ...\n",
            "3666    so only blokes drink beer sorry but if you are...\n",
            "3667    new to the shelf this week look forward to rea...\n",
            "Name: tweet, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(df['tweet'].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU2q_4tGNU2W"
      },
      "source": [
        "# Task 3: Text Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmxLWqu01gZd"
      },
      "source": [
        "## Splitting the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LraZe2wH1kjS"
      },
      "source": [
        "Here we split the dataset in train, test and validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ug_gd5gz1j5S",
        "outputId": "33a847ec-b9dd-4d7e-e94b-5aaadf91e89f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: (2870, 4)\n",
            "Test size: (286, 4)\n",
            "Validation size: (158, 4)\n"
          ]
        }
      ],
      "source": [
        "df_train = df.loc[df['split'] == \"training\"].drop(columns=['split'])\n",
        "df_test = df.loc[df['split'] == \"test\"].drop(columns=['split'])\n",
        "df_val = df.loc[df['split'] == \"validation\"].drop(columns=['split'])\n",
        "\n",
        "print(f\"Train size: {df_train.shape}\")\n",
        "print(f\"Test size: {df_test.shape}\")\n",
        "print(f\"Validation size: {df_val.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wCba8rlw8lK"
      },
      "source": [
        "### Try to use Tokenizer to build the vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQmHVeFUnb8Y"
      },
      "source": [
        " ## Vocabulary Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AayUlbam3-Rm"
      },
      "source": [
        "We create the vocabulary only using the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "7HbdgvPZnpxr"
      },
      "outputs": [],
      "source": [
        "def build_vocabulary(df: pd.DataFrame) -> (Dict[int, str], Dict[str, int], List[str]):\n",
        "    \"\"\"\n",
        "    Given a dataset, builds the corresponding word vocabulary.\n",
        "\n",
        "    :param df: dataset from which we want to build the word vocabulary (pandas.DataFrame)\n",
        "    :return:\n",
        "      - word vocabulary: vocabulary index to word\n",
        "      - inverse word vocabulary: word to vocabulary index\n",
        "      - word listing: set of unique terms that build up the vocabulary\n",
        "    \"\"\"\n",
        "    idx_to_word = OrderedDict()\n",
        "    word_to_idx = OrderedDict()\n",
        "\n",
        "    curr_idx = 0\n",
        "    # '''\n",
        "    # Here we add the special token [UNK] to our vocabulary\n",
        "    word_to_idx[\"[UNK]\"] = curr_idx\n",
        "    idx_to_word[curr_idx] = \"[UNK]\"\n",
        "    curr_idx += 1\n",
        "    word_to_idx[\"[PAD]\"] = curr_idx         #add the PAD index\n",
        "    idx_to_word[curr_idx] = \"[PAD]\"\n",
        "    curr_idx += 1\n",
        "    # '''\n",
        "    for sentence in tqdm(df.tweet.values):\n",
        "        tokens = sentence.split()\n",
        "        for token in tokens:\n",
        "            if token not in word_to_idx:\n",
        "                word_to_idx[token] = curr_idx\n",
        "                idx_to_word[curr_idx] = token\n",
        "                curr_idx += 1\n",
        "\n",
        "\n",
        "\n",
        "    word_listing = list(idx_to_word.values())\n",
        "    return idx_to_word, word_to_idx, word_listing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anfsmQRWoIzi",
        "outputId": "54a9cca8-ce9a-4b81-fe76-706a230f1851"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2870/2870 [00:00<00:00, 182289.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Debug] Index -> Word vocabulary size: 9820\n",
            "[Debug] Word -> Index vocabulary size: 9820\n",
            "[Debug] Some words: [('[UNK]', 0), ('[PAD]', 1), ('write', 2), ('a', 3), ('uni', 4), ('essay', 5), ('in', 6), ('my', 7), ('local', 8), ('pub', 9)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "idx_to_word_train, word_to_idx_train, word_listing_train = build_vocabulary(df_train)\n",
        "\n",
        "print(f'[Debug] Index -> Word vocabulary size: {len(idx_to_word_train)}')\n",
        "print(f'[Debug] Word -> Index vocabulary size: {len(word_to_idx_train)}')\n",
        "print(f'[Debug] Some words: {[(idx_to_word_train[idx], idx) for idx in np.arange(10) ]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsjJBA41okTn"
      },
      "source": [
        "### Vocabulary Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "UqvEPsy8ocTT"
      },
      "outputs": [],
      "source": [
        "def evaluate_vocabulary(idx_to_word: Dict[int, str], word_to_idx: Dict[str, int],\n",
        "                        word_listing: List[str], df: pd.DataFrame, check_default_size: bool = False):\n",
        "    print(\"[Vocabulary Evaluation] Size checking...\")\n",
        "    assert len(idx_to_word) == len(word_to_idx)\n",
        "    assert len(idx_to_word) == len(word_listing)\n",
        "\n",
        "    print(\"[Vocabulary Evaluation] Content checking...\")\n",
        "    for i in tqdm(range(len(idx_to_word))):\n",
        "        assert idx_to_word[i] in word_to_idx\n",
        "        assert word_to_idx[idx_to_word[i]] == i\n",
        "\n",
        "    print(\"[Vocabulary Evaluation] Consistency checking...\")\n",
        "    _, _, first_word_listing = build_vocabulary(df)\n",
        "    _, _, second_word_listing = build_vocabulary(df)\n",
        "    assert first_word_listing == second_word_listing\n",
        "\n",
        "    print(\"[Vocabulary Evaluation] Toy example checking...\")\n",
        "    toy_df = pd.DataFrame.from_dict({\n",
        "        'tweet': [\"all that glitters is not gold\", \"all in all i like this assignment\"]\n",
        "    })\n",
        "    _, _, toy_word_listing = build_vocabulary(toy_df)\n",
        "    toy_valid_vocabulary = set(' '.join(toy_df.tweet.values).split())\n",
        "    # Includi anche [UNK] nel confronto\n",
        "    toy_valid_vocabulary.add(\"[UNK]\")\n",
        "    toy_valid_vocabulary.add(\"[PAD]\")           #add pad to the test \n",
        "    assert set(toy_word_listing) == toy_valid_vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hb7Eh1gooTu",
        "outputId": "3afcabfc-50b5-4d3f-dd3b-362a0c05c102"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary evaluation...\n",
            "[Vocabulary Evaluation] Size checking...\n",
            "[Vocabulary Evaluation] Content checking...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9820/9820 [00:00<00:00, 2398001.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Vocabulary Evaluation] Consistency checking...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2870/2870 [00:00<00:00, 347146.51it/s]\n",
            "100%|██████████| 2870/2870 [00:00<00:00, 164502.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Vocabulary Evaluation] Toy example checking...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation completed!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Vocabulary evaluation...\")\n",
        "evaluate_vocabulary(idx_to_word_train, word_to_idx_train, word_listing_train, df_train)\n",
        "print(\"Evaluation completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYzT9t9Gqj3f"
      },
      "source": [
        "## Saving Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVMJ5eRtqv0N",
        "outputId": "48e7e41d-e41a-4098-c341-a01c9fb56b95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving vocabulary to c:\\Users\\morne\\Desktop\\ProjectNLP\\git_repository\\NLP_Assignments\\Datasets\\vocab.json\n",
            "Saving completed!\n"
          ]
        }
      ],
      "source": [
        "vocab_path = Path.cwd().joinpath('Datasets', 'vocab.json')\n",
        "\n",
        "print(f\"Saving vocabulary to {vocab_path}\")\n",
        "with vocab_path.open(mode='w') as f:\n",
        "    sj.dump(word_to_idx_train, f, indent=4)\n",
        "print(\"Saving completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MS28pC08ztV"
      },
      "source": [
        "## GloVe embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "fhIaF3QG8_Tf"
      },
      "outputs": [],
      "source": [
        "def load_embedding_model(model_type: str,\n",
        "                         embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
        "    \"\"\"\n",
        "    Loads a pre-trained word embedding model via gensim library.\n",
        "\n",
        "    :param model_type: name of the word embedding model to load.\n",
        "    :param embedding_dimension: size of the embedding space to consider\n",
        "\n",
        "    :return\n",
        "        - pre-trained word embedding model (gensim KeyedVectors object)\n",
        "    \"\"\"\n",
        "    download_path = \"\"\n",
        "\n",
        "    if model_type.strip().lower() == 'glove':\n",
        "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
        "\n",
        "    else:\n",
        "        raise AttributeError(\"Unsupported embedding model type! Available ones: word2vec, glove, fasttext\")\n",
        "\n",
        "    try:\n",
        "        emb_model = gloader.load(download_path)\n",
        "    except ValueError as e:\n",
        "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
        "        print(\"Glove: 50, 100, 200, 300\")\n",
        "        raise e\n",
        "\n",
        "    return emb_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kT3IwEKQ9JSU",
        "outputId": "b75b2a75-3bd4-448d-9b83-2a18570fd08e"
      },
      "outputs": [],
      "source": [
        "embedding_model = load_embedding_model(model_type=\"glove\",\n",
        "                                       embedding_dimension=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "gbxw6ndV_NQf"
      },
      "outputs": [],
      "source": [
        "def check_OOV_terms(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
        "                    word_listing: List[str]):\n",
        "    \"\"\"\n",
        "    Checks differences between pre-trained embedding model vocabulary\n",
        "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param word_listing: dataset specific vocabulary (list)\n",
        "\n",
        "    :return\n",
        "        - list of OOV terms\n",
        "    \"\"\"\n",
        "    embedding_vocabulary = set(embedding_model.key_to_index.keys())\n",
        "    oov = set(word_listing).difference(embedding_vocabulary)\n",
        "    return list(oov)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W783gsHF_QD4",
        "outputId": "b37e49dd-5023-40e9-c541-8d487c3bdec2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total OOV terms in training set: 1842 (18.76%)\n"
          ]
        }
      ],
      "source": [
        "oov_terms_train = check_OOV_terms(embedding_model, word_listing_train)\n",
        "oov_percentage_train = float(len(oov_terms_train)) * 100 / len(word_listing_train)\n",
        "print(f\"Total OOV terms in training set: {len(oov_terms_train)} ({oov_percentage_train:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RKUpyOzy4-y"
      },
      "source": [
        "Try to use Blob correct in order to correct spelling errors in tweets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jWTz4quZxHP"
      },
      "source": [
        "### Handling train dataset OOV terms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EFIJbpwZ9DA"
      },
      "source": [
        "We have to put all the unseen tokens in the train dataset in our vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "g5n39svJA2SZ"
      },
      "outputs": [],
      "source": [
        "def build_embedding_matrix(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
        "                           embedding_dimension: int,\n",
        "                           word_to_idx: Dict[str, int],\n",
        "                           vocab_size: int,\n",
        "                           oov_terms: List[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    :param vocab_size: size of the vocabulary\n",
        "    :param oov_terms: list of OOV terms (list)\n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dimension), dtype=np.float32)\n",
        "    for word, idx in tqdm(word_to_idx.items()):\n",
        "        try:\n",
        "            embedding_vector = embedding_model[word]\n",
        "        except (KeyError, TypeError):\n",
        "          if word == '[UNK]':\n",
        "            # we assign a random embedding to the [UNK] token\n",
        "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
        "          else:\n",
        "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
        "\n",
        "        embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "    return embedding_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQcy1MNsA4mi",
        "outputId": "e62ce2be-d645-4965-c0ae-7458af193650"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9820/9820 [00:00<00:00, 307018.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Embedding matrix shape: (9820, 50)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Testing\n",
        "embedding_dimension = 50\n",
        "embedding_matrix = build_embedding_matrix(embedding_model, embedding_dimension, word_to_idx_train, len(word_to_idx_train), oov_terms_train)\n",
        "print(f\"\\nEmbedding matrix shape: {embedding_matrix.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TASK4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "277\n",
            "9820\n"
          ]
        }
      ],
      "source": [
        "#non mi convince da rivedere\n",
        "max_tweet_length=0\n",
        "for t in df['tweet']:\n",
        "    if len(t)>max_tweet_length:\n",
        "        max_tweet_length=len(t)\n",
        "print(max_tweet_length)\n",
        "print(len(word_to_idx_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder_embedding (Embeddin  (None, None, 50)         491000    \n",
            " g)                                                              \n",
            "                                                                 \n",
            " bidirectional_12 (Bidirecti  (None, 256)              183296    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 674,553\n",
            "Trainable params: 674,553\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder_embedding (Embeddin  (None, None, 50)         491000    \n",
            " g)                                                              \n",
            "                                                                 \n",
            " bidirectional_13 (Bidirecti  (None, None, 256)        183296    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " lstm_31 (LSTM)              (None, 128)               197120    \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 871,545\n",
            "Trainable params: 871,545\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "\n",
        "def createModelBase(word_to_idx_train,embedding_dimension,embedding_matrix,units_dim=128):\n",
        "\n",
        "    embedding = tf.keras.layers.Embedding(input_dim=len(word_to_idx_train),\n",
        "                                        output_dim=embedding_dimension,\n",
        "                                        weights=[embedding_matrix],\n",
        "                                        mask_zero=True,                   # automatically masks padding tokens\n",
        "                                        name='encoder_embedding')\n",
        "    model = Sequential()\n",
        "    model.add(embedding)\n",
        "    model.add(Bidirectional(LSTM(units=units_dim)))\n",
        "    model.add(Dense(units=1, activation='sigmoid'))  # Attivazione sigmoid per classificazione binaria\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "def createModel1(word_to_idx_train,embedding_dimension,embedding_matrix,units_dim=128):\n",
        "\n",
        "    embedding = tf.keras.layers.Embedding(input_dim=len(word_to_idx_train),\n",
        "                                        output_dim=embedding_dimension,\n",
        "                                        weights=[embedding_matrix],\n",
        "                                        mask_zero=True,                   # automatically masks padding tokens\n",
        "                                        name='encoder_embedding')\n",
        "    model = Sequential()\n",
        "    model.add(embedding)\n",
        "    model.add(Bidirectional(LSTM(units=units_dim,return_sequences=True)))\n",
        "    model.add(LSTM(units=units_dim))\n",
        "    model.add(Dense(units=1, activation='sigmoid'))  # Attivazione sigmoid per classificazione binaria\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "model_base=createModelBase(word_to_idx_train,embedding_dimension,embedding_matrix)\n",
        "model_1=createModel1(word_to_idx_train,embedding_dimension,embedding_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TASK5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def prepare_input(data, vocabulary):\n",
        "    X=[]\n",
        "    for tokens in data['tweet']:\n",
        "        indices = []\n",
        "        for i in range(max_tweet_length):\n",
        "            if i <len(tokens):\n",
        "                if tokens[i] in vocabulary:\n",
        "                    indices.append(vocabulary[tokens[i]])\n",
        "                else:\n",
        "                    # Handle out-of-vocabulary tokens if necessary\n",
        "                    indices.append(vocabulary['[UNK]'])  # Example: -1 for unknown tokens\n",
        "            else: \n",
        "                indices.append(vocabulary['[PAD]'])\n",
        "        X.append(indices)\n",
        "        \n",
        "    \n",
        "    return np.array(X),np.array(data['hard_label_task1'])\n",
        "\n",
        "\n",
        "X_train,y_train=prepare_input(df_train,word_to_idx_train)\n",
        "X_val,y_val=prepare_input(df_val,word_to_idx_train)      #####qui non so se modificare word_to_idx_train con uno apposta per il validation e il test\n",
        "X_test,y_test=prepare_input(df_test,word_to_idx_train)    #####qui non so se modificare word_to_idx_train con uno apposta per il validation e il test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            " 5/90 [>.............................] - ETA: 56s - loss: 0.6116 - accuracy: 0.7375"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[153], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m####prova di fit da eliminare\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel_base\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m model_base\u001b[38;5;241m.\u001b[39mevaluate(X_test,y_test)\n",
            "File \u001b[1;32mc:\\Users\\morne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\morne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
            "File \u001b[1;32mc:\\Users\\morne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\morne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32mc:\\Users\\morne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
            "File \u001b[1;32mc:\\Users\\morne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\morne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[1;32mc:\\Users\\morne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
            "File \u001b[1;32mc:\\Users\\morne\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "                    ####prova di fit da eliminare\n",
        "model_base.fit(X_train,y_train,epochs=2,validation_data=(X_val,y_val),shuffle=True)\n",
        "model_base.evaluate(X_test,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating models with seed 42\n",
            "Training and evaluating model: model_base\n",
            "Epoch 1/2\n",
            "90/90 [==============================] - 56s 619ms/step - loss: 0.6585 - accuracy: 0.6052\n",
            "Epoch 2/2\n",
            "90/90 [==============================] - 57s 630ms/step - loss: 0.6534 - accuracy: 0.6251\n",
            "5/5 [==============================] - 1s 185ms/step\n",
            "Training and evaluating model: model_1\n",
            "Epoch 1/2\n",
            "90/90 [==============================] - 115s 1s/step - loss: 0.6748 - accuracy: 0.6038\n",
            "Epoch 2/2\n",
            "90/90 [==============================] - 117s 1s/step - loss: 0.6731 - accuracy: 0.6042\n",
            "5/5 [==============================] - 8s 321ms/step\n",
            "Evaluating models with seed 100\n",
            "Training and evaluating model: model_base\n",
            "Epoch 1/2\n",
            "90/90 [==============================] - 59s 655ms/step - loss: 0.6578 - accuracy: 0.6084\n",
            "Epoch 2/2\n",
            "90/90 [==============================] - 63s 698ms/step - loss: 0.6547 - accuracy: 0.6164\n",
            "5/5 [==============================] - 2s 315ms/step\n",
            "Training and evaluating model: model_1\n",
            "Epoch 1/2\n",
            "90/90 [==============================] - 130s 1s/step - loss: 0.6715 - accuracy: 0.6042\n",
            "Epoch 2/2\n",
            "90/90 [==============================] - 137s 2s/step - loss: 0.6714 - accuracy: 0.6038\n",
            "5/5 [==============================] - 2s 483ms/step\n",
            "Evaluating models with seed 2023\n",
            "Training and evaluating model: model_base\n",
            "Epoch 1/2\n",
            "90/90 [==============================] - 60s 661ms/step - loss: 0.6419 - accuracy: 0.6247\n",
            "Epoch 2/2\n",
            "90/90 [==============================] - 58s 643ms/step - loss: 0.6482 - accuracy: 0.6265\n",
            "5/5 [==============================] - 1s 289ms/step\n",
            "Training and evaluating model: model_1\n",
            "Epoch 1/2\n",
            "90/90 [==============================] - 111s 1s/step - loss: 0.6717 - accuracy: 0.6042\n",
            "Epoch 2/2\n",
            "90/90 [==============================] - 111s 1s/step - loss: 0.6714 - accuracy: 0.6042\n",
            "5/5 [==============================] - 2s 435ms/step\n",
            "Best model: model_base with macro F1-score: 0.5110\n",
            "Seed: 42, Model: model_base, Macro F1: 0.3968\n",
            "Seed: 42, Model: model_1, Macro F1: 0.3629\n",
            "Seed: 100, Model: model_base, Macro F1: 0.3936\n",
            "Seed: 100, Model: model_1, Macro F1: 0.3629\n",
            "Seed: 2023, Model: model_base, Macro F1: 0.5110\n",
            "Seed: 2023, Model: model_1, Macro F1: 0.3629\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Define a function to train a model\n",
        "def train_model(model, X_train, y_train,epochs):\n",
        "    model.fit(X_train, y_train,epochs=epochs,batch_size=64)\n",
        "    return model\n",
        "\n",
        "# Define a function to evaluate a model\n",
        "def evaluate_model(model, X_val, y_val):\n",
        "    y_pred = model.predict(X_val)\n",
        "    y_pred = (y_pred >= 0.5).astype(int)  # Convert continuous outputs to binary labels\n",
        "    macro_f1 = f1_score(y_val, y_pred, average='macro')\n",
        "    return macro_f1\n",
        "\n",
        "# Main function to handle tasks\n",
        "def train_and_evaluate(models, X_train, y_train,X_val,y_val,epochs=2, seeds=[42, 100, 2023]):\n",
        "    results = []\n",
        "\n",
        "    # Loop over seeds\n",
        "    for seed in seeds:\n",
        "        print(f\"Evaluating models with seed {seed}\")\n",
        "        np.random.seed(seed)\n",
        "       \n",
        "        # Split the dataset\n",
        "        \n",
        "\n",
        "        # Train and evaluate each model\n",
        "        for model_name, model in models.items():\n",
        "            print(f\"Training and evaluating model: {model_name}\")\n",
        "\n",
        "            # Train the model\n",
        "            trained_model = train_model(model, X_train, y_train,epochs)\n",
        "\n",
        "            # Evaluate the model\n",
        "            macro_f1 = evaluate_model(trained_model, X_val, y_val)\n",
        "            \n",
        "            # Store results\n",
        "            results.append({\n",
        "                'seed': seed,\n",
        "                'model_name': model_name,\n",
        "                'macro_f1': macro_f1\n",
        "            })\n",
        "\n",
        "    # Aggregate results to find the best model\n",
        "    best_model = max(results, key=lambda x: x['macro_f1'])\n",
        "    print(f\"Best model: {best_model['model_name']} with macro F1-score: {best_model['macro_f1']:.4f}\")\n",
        "\n",
        "    return results, best_model\n",
        "\n",
        "models = {\n",
        "    'model_base':model_base,\n",
        "    'model_1': model_1\n",
        "}\n",
        "# Train and evaluate models\n",
        "results, best_model = train_and_evaluate(models, X_train,y_train,X_val,y_val,epochs=2)\n",
        "\n",
        "# Print detailed results\n",
        "for result in results:\n",
        "    print(f\"Seed: {result['seed']}, Model: {result['model_name']}, Macro F1: {result['macro_f1']:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
