{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "sz13BvtmN49a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n",
        "!pip install nltk\n",
        "!pip install tqdm\n",
        "!pip install simplejson"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WE9zjHoMn6h",
        "outputId": "6c849bdb-2940-4f61-ad93-1018a2e5c332"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.10/dist-packages (3.19.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "C8iazcwMNM8Q"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import shutil\n",
        "import json\n",
        "import urllib\n",
        "import tarfile\n",
        "from pathlib import Path\n",
        "import simplejson as sj\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from typing import Dict, OrderedDict, List\n",
        "\n",
        "import re\n",
        "import emoji\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "import gensim\n",
        "import gensim.downloader as gloader\n",
        "\n",
        "\n",
        "\n",
        "from typing import Iterable\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Corpus"
      ],
      "metadata": {
        "id": "aorcZzjLOQII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Downloading the dataset"
      ],
      "metadata": {
        "id": "S7xVZIPFOXPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all we need to **download** the `A1/data` folder."
      ],
      "metadata": {
        "id": "vz0BiOVBOhui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DownloadProgressBar(tqdm):\n",
        "    def update_to(self, b=1, bsize=1, tsize=None):\n",
        "        if tsize is not None:\n",
        "            self.total = tsize\n",
        "        self.update(b * bsize - self.n)\n",
        "\n",
        "def download_url(download_path: Path, url: str):\n",
        "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
        "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
        "        urllib.request.urlretrieve(url, filename=download_path, reporthook=t.update_to)"
      ],
      "metadata": {
        "id": "5CFbK72BOtVu"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_dataset(download_path: Path, url: str):\n",
        "    print(\"Downloading dataset...\")\n",
        "    download_url(url=url, download_path=download_path)\n",
        "    print(\"Download complete!\")"
      ],
      "metadata": {
        "id": "4Jwr7Ns6Ot1i"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we put all the urls\n",
        "urls = {\n",
        "    \"training\": \"https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/main/2024-2025/Assignment%201/data/training.json\",\n",
        "    \"test\": \"https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/main/2024-2025/Assignment%201/data/test.json\",\n",
        "    \"validation\": \"https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/main/2024-2025/Assignment%201/data/validation.json\"\n",
        "}"
      ],
      "metadata": {
        "id": "jsZv2RSOViC8"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Current work directory: {Path.cwd()}\")\n",
        "dataset_folder = Path.cwd().joinpath(\"Datasets\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NvOZ5g2O8Ra",
        "outputId": "f567f9be-9600-4a03-80dc-42124b162ad5"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current work directory: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not dataset_folder.exists():\n",
        "    dataset_folder.mkdir(parents=True)"
      ],
      "metadata": {
        "id": "mGQb4TxtRYYm"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, url in urls.items():\n",
        "    download_path = dataset_folder.joinpath(f\"{name}.json\")\n",
        "    download_dataset(download_path, url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r99L1rTzV12C",
        "outputId": "bd1b277a-46e9-4fde-8c5e-91b0a675993d"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training.json: 6.23MB [00:00, 61.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete!\n",
            "Downloading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test.json: 500kB [00:00, 7.71MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete!\n",
            "Downloading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "validation.json: 1.16MB [00:00, 5.82MB/s]                            "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load the three JSON files and encode them as pandas dataframes."
      ],
      "metadata": {
        "id": "97p0MGpIYdGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_json_file(file_path: Path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)"
      ],
      "metadata": {
        "id": "tjPonkkIduMr"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe_rows = []\n",
        "\n",
        "\n",
        "for name, url in urls.items():\n",
        "    # per ogni file creiamo il file_path e leggiamo il file\n",
        "    file_path = dataset_folder.joinpath(f\"{name}.json\")\n",
        "\n",
        "    json_data = load_json_file(file_path)\n",
        "\n",
        "    # per ogni chiave nel json_data creo una dataframe_row\n",
        "    for key in json_data.keys():\n",
        "        df_row = json_data[key]\n",
        "        df_row[\"split\"] = name\n",
        "        dataframe_rows.append(df_row)\n"
      ],
      "metadata": {
        "id": "mUz9IDNUcb7J"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder = Path.cwd().joinpath(\"Datasets\", \"Dataframes\")\n",
        "if not folder.exists():\n",
        "    folder.mkdir(parents=True)\n",
        "\n",
        "\n",
        "# transform the list of rows in a proper dataframe\n",
        "df = pd.DataFrame(dataframe_rows)\n",
        "\n",
        "for name, url in urls.items():\n",
        "  df_path = folder.with_name(name + \".pkl\")\n",
        "  df.to_pickle(df_path)"
      ],
      "metadata": {
        "id": "u2xd6biKiKbW"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Generate hard labels"
      ],
      "metadata": {
        "id": "aegG34haxrJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate hard labels for Task 1 using majority voting and store them in a new dataframe column called `hard_label_task1`. Items without a clear majority will be removed from the dataset."
      ],
      "metadata": {
        "id": "c6VZjWS-xvb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_majority_voting(labels: list):\n",
        "\n",
        "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
        "    majority_label = np.argwhere(counts == np.max(counts))\n",
        "\n",
        "    majority_label = unique_labels[majority_label].flatten().tolist()\n",
        "\n",
        "    if len(majority_label) > 1:\n",
        "        majority_label = None\n",
        "\n",
        "\n",
        "    return majority_label"
      ],
      "metadata": {
        "id": "sORAVONvpU_v"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_hard_labels(df):\n",
        "    hard_labels = []\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        # Estrai le etichette dalla colonna 'labels_task1'\n",
        "        labels = row['labels_task1']\n",
        "        # print(labels)\n",
        "\n",
        "        # Verifica se 'labels' è una lista e contiene elementi\n",
        "        if isinstance(labels, list) and len(labels) > 0:\n",
        "            # Calcola la moda (voto di maggioranza)\n",
        "            most_common_label = compute_majority_voting(labels)\n",
        "            # print(most_common_label)\n",
        "            hard_labels.append(most_common_label)\n",
        "\n",
        "    # Aggiungi le hard labels come nuova colonna\n",
        "    df['hard_label_task1'] = hard_labels\n",
        "\n",
        "    # Rimuovi le righe senza una chiara maggioranza (se necessario)\n",
        "    df = df[df['hard_label_task1'].notnull()]\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "0LP9rRC1zG0Q"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = generate_hard_labels(df)\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckDgdopDz3d2",
        "outputId": "65dd6169-843c-435a-96a3-004c6b61be52"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  id_EXIST lang                                              tweet  \\\n",
            "0   100001   es  @TheChiflis Ignora al otro, es un capullo.El p...   \n",
            "1   100002   es  @ultimonomada_ Si comicsgate se parece en algo...   \n",
            "2   100003   es  @Steven2897 Lee sobre Gamergate, y como eso ha...   \n",
            "4   100005   es  @novadragon21 @icep4ck @TvDannyZ Entonces como...   \n",
            "5   100006   es  @yonkykong Aaah sí. Andrew Dobson. El que se d...   \n",
            "\n",
            "   number_annotators                                         annotators  \\\n",
            "0                  6  [Annotator_1, Annotator_2, Annotator_3, Annota...   \n",
            "1                  6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
            "2                  6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
            "4                  6  [Annotator_19, Annotator_20, Annotator_21, Ann...   \n",
            "5                  6  [Annotator_25, Annotator_26, Annotator_27, Ann...   \n",
            "\n",
            "    gender_annotators                          age_annotators  \\\n",
            "0  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
            "1  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
            "2  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
            "4  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
            "5  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
            "\n",
            "                    labels_task1  \\\n",
            "0  [YES, YES, NO, YES, YES, YES]   \n",
            "1      [NO, NO, NO, NO, YES, NO]   \n",
            "2       [NO, NO, NO, NO, NO, NO]   \n",
            "4   [YES, NO, YES, NO, YES, YES]   \n",
            "5       [NO, NO, NO, NO, NO, NO]   \n",
            "\n",
            "                                        labels_task2  \\\n",
            "0  [REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...   \n",
            "1                            [-, -, -, -, DIRECT, -]   \n",
            "2                                 [-, -, -, -, -, -]   \n",
            "4  [REPORTED, -, JUDGEMENTAL, -, JUDGEMENTAL, DIR...   \n",
            "5                                 [-, -, -, -, -, -]   \n",
            "\n",
            "                                        labels_task3     split  \\\n",
            "0  [[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...  training   \n",
            "1       [[-], [-], [-], [-], [OBJECTIFICATION], [-]]  training   \n",
            "2                     [[-], [-], [-], [-], [-], [-]]  training   \n",
            "4  [[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [-...  training   \n",
            "5                     [[-], [-], [-], [-], [-], [-]]  training   \n",
            "\n",
            "  hard_label_task1  \n",
            "0            [YES]  \n",
            "1             [NO]  \n",
            "2             [NO]  \n",
            "4            [YES]  \n",
            "5             [NO]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Filter the DataFrame"
      ],
      "metadata": {
        "id": "-VdYgdgwqsNu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter the DataFrame to keep only rows where the `lang` column is `'en'`."
      ],
      "metadata": {
        "id": "QVfoZzusqy0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['lang'] == 'en']\n",
        "print(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WYAWKilq1tl",
        "outputId": "ca75b8d0-78e1-495e-b38e-85aa8ae9871d"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3314, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Remove unwanted columns"
      ],
      "metadata": {
        "id": "etQQV0YTrN9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keep only `id_EXIST`, `lang`, `tweet`, and `hard_label_task1`."
      ],
      "metadata": {
        "id": "T4nDBRNurTT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_unwanted_columns(df):\n",
        "\n",
        "    columns_to_keep = ['id_EXIST', 'lang', 'tweet', 'hard_label_task1', 'split']\n",
        "    df = df[columns_to_keep]\n",
        "    return df"
      ],
      "metadata": {
        "id": "hb0-lUqPrZEH"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = remove_unwanted_columns(df)"
      ],
      "metadata": {
        "id": "YoZB0G8xrgcA"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Encode the hard_label_task1 column"
      ],
      "metadata": {
        "id": "68p9vTuLru-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use 1 to represent \"YES\" and 0 to represent \"NO\" in the `hard_label_task1 column`."
      ],
      "metadata": {
        "id": "70CeoJ1hr-rB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['hard_label_task1'] = df['hard_label_task1'].apply(lambda x: 1 if x[0] == 'YES' else 0)\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lh8WC291tNGA",
        "outputId": "2d609029-575a-40f3-b889-7eaee15c5cc8"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     id_EXIST lang                                              tweet  \\\n",
            "3661   200002   en  Writing a uni essay in my local pub with a cof...   \n",
            "3662   200003   en  @UniversalORL it is 2021 not 1921. I dont appr...   \n",
            "3665   200006   en  According to a customer I have plenty of time ...   \n",
            "3666   200007   en  So only 'blokes' drink beer? Sorry, but if you...   \n",
            "3667   200008   en  New to the shelves this week - looking forward...   \n",
            "\n",
            "      hard_label_task1     split  \n",
            "3661                 1  training  \n",
            "3662                 1  training  \n",
            "3665                 1  training  \n",
            "3666                 1  training  \n",
            "3667                 0  training  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Data Cleaning"
      ],
      "metadata": {
        "id": "qyAK2RBGu8Bq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9rsrT-WJDoK",
        "outputId": "6611141a-42fb-435a-cb04-3917ba830cdd"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check this link: [all existing emojis](https://www.unicode.org/Public/emoji/1.0//emoji-data.txt). And also this: [emojis unicode consortium](https://unicode.org/emoji/charts/full-emoji-list.html)."
      ],
      "metadata": {
        "id": "W3yM7nuc_DpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lower(text):\n",
        "    return text.lower()"
      ],
      "metadata": {
        "id": "GyDbd9BxU31A"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_emoji(text):\n",
        "    return emoji.replace_emoji(text, replace='')"
      ],
      "metadata": {
        "id": "2U3Eg7_4_-Ic"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_hastag(text):\n",
        "    at = re.compile(r'#\\S+')\n",
        "    return at.sub(r'',text)"
      ],
      "metadata": {
        "id": "69qq6Fe25xMc"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_mention(text):\n",
        "    at = re.compile(r'@\\S+')\n",
        "    return at.sub(r'',text)"
      ],
      "metadata": {
        "id": "rp8HhWlh5NEu"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_URL(text):\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url.sub(r'',text)"
      ],
      "metadata": {
        "id": "GwVvkIsa64wk"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_special_characters(text):\n",
        "    pattern = r'[^a-zA-Z0-9\\s]'\n",
        "    return re.sub(pattern, '', text)"
      ],
      "metadata": {
        "id": "iLRDqOqS7QKt"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_quotes(text):\n",
        "    pattern = r'^\"|\"$‘’'\n",
        "    return re.sub(pattern, '', text)"
      ],
      "metadata": {
        "id": "v__lt8yu8Oap"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_extra_spaces(text):\n",
        "    text.strip()\n",
        "    pattern = r'\\s+'\n",
        "    return re.sub(pattern, ' ', text)"
      ],
      "metadata": {
        "id": "6aRkZPTP9KG0"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "    def get_wordnet_key(pos_tag):\n",
        "        if pos_tag.startswith('J'):\n",
        "            return wordnet.ADJ\n",
        "        elif pos_tag.startswith('V'):\n",
        "            return wordnet.VERB\n",
        "        elif pos_tag.startswith('N'):\n",
        "            return wordnet.NOUN\n",
        "        elif pos_tag.startswith('R'):\n",
        "            return wordnet.ADV\n",
        "        else:\n",
        "            return 'n'\n",
        "\n",
        "\n",
        "    def lem_text(text: str):\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "        tagged = pos_tag(tokens)\n",
        "        words = [lemmatizer.lemmatize(word, get_wordnet_key(tag)) for word, tag in tagged]\n",
        "        return \" \".join(words)\n",
        "\n",
        "\n",
        "    return lem_text(text)"
      ],
      "metadata": {
        "id": "Eb2ni1lLI4oH"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_tweet(tweet: str):\n",
        "    tweet = lower(tweet)\n",
        "    tweet = remove_emoji(tweet)\n",
        "    tweet = remove_hastag(tweet)\n",
        "    tweet = remove_mention(tweet)\n",
        "    tweet = remove_URL(tweet)\n",
        "    tweet = remove_special_characters(tweet)\n",
        "    tweet = remove_quotes(tweet)\n",
        "    tweet = remove_extra_spaces(tweet)\n",
        "    tweet = lemmatize(tweet)\n",
        "    return tweet"
      ],
      "metadata": {
        "id": "Xid-F6PgLxoN"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['tweet'] = df['tweet'].apply(clean_tweet)"
      ],
      "metadata": {
        "id": "93acNCscMDRL"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['tweet'].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcwvF65MNAXd",
        "outputId": "1b1ba20f-e91e-43f6-e0da-dc6b9caffa1c"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3661    write a uni essay in my local pub with a coffe...\n",
            "3662    it be 2021 not 1921 i dont appreciate that on ...\n",
            "3665    accord to a customer i have plenty of time to ...\n",
            "3666    so only blokes drink beer sorry but if you are...\n",
            "3667    new to the shelf this week look forward to rea...\n",
            "Name: tweet, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Text Encoding"
      ],
      "metadata": {
        "id": "hU2q_4tGNU2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting the dataset"
      ],
      "metadata": {
        "id": "PmxLWqu01gZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we split the dataset in train, test and validation."
      ],
      "metadata": {
        "id": "LraZe2wH1kjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df.loc[df['split'] == \"training\"].drop(columns=['split'])\n",
        "df_test = df.loc[df['split'] == \"test\"].drop(columns=['split'])\n",
        "df_val = df.loc[df['split'] == \"validation\"].drop(columns=['split'])\n",
        "\n",
        "print(f\"Train size: {df_train.shape}\")\n",
        "print(f\"Test size: {df_test.shape}\")\n",
        "print(f\"Validation size: {df_val.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ug_gd5gz1j5S",
        "outputId": "33a847ec-b9dd-4d7e-e94b-5aaadf91e89f"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: (2870, 4)\n",
            "Test size: (286, 4)\n",
            "Validation size: (158, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try to use Tokenizer to build the vocabulary"
      ],
      "metadata": {
        "id": "8wCba8rlw8lK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Vocabulary Creation"
      ],
      "metadata": {
        "id": "lQmHVeFUnb8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create the vocabulary only using the training dataset."
      ],
      "metadata": {
        "id": "AayUlbam3-Rm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocabulary(df: pd.DataFrame) -> (Dict[int, str], Dict[str, int], List[str]):\n",
        "    \"\"\"\n",
        "    Given a dataset, builds the corresponding word vocabulary.\n",
        "\n",
        "    :param df: dataset from which we want to build the word vocabulary (pandas.DataFrame)\n",
        "    :return:\n",
        "      - word vocabulary: vocabulary index to word\n",
        "      - inverse word vocabulary: word to vocabulary index\n",
        "      - word listing: set of unique terms that build up the vocabulary\n",
        "    \"\"\"\n",
        "    idx_to_word = OrderedDict()\n",
        "    word_to_idx = OrderedDict()\n",
        "\n",
        "    curr_idx = 0\n",
        "    # '''\n",
        "    # Here we add the special token [UNK] to our vocabulary\n",
        "    word_to_idx[\"[UNK]\"] = curr_idx\n",
        "    idx_to_word[curr_idx] = \"[UNK]\"\n",
        "    curr_idx += 1\n",
        "    # '''\n",
        "    for sentence in tqdm(df.tweet.values):\n",
        "        tokens = sentence.split()\n",
        "        for token in tokens:\n",
        "            if token not in word_to_idx:\n",
        "                word_to_idx[token] = curr_idx\n",
        "                idx_to_word[curr_idx] = token\n",
        "                curr_idx += 1\n",
        "\n",
        "\n",
        "\n",
        "    word_listing = list(idx_to_word.values())\n",
        "    return idx_to_word, word_to_idx, word_listing"
      ],
      "metadata": {
        "id": "7HbdgvPZnpxr"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_to_word_train, word_to_idx_train, word_listing_train = build_vocabulary(df_train)\n",
        "\n",
        "print(f'[Debug] Index -> Word vocabulary size: {len(idx_to_word_train)}')\n",
        "print(f'[Debug] Word -> Index vocabulary size: {len(word_to_idx_train)}')\n",
        "print(f'[Debug] Some words: {[(idx_to_word_train[idx], idx) for idx in np.arange(10) + 1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anfsmQRWoIzi",
        "outputId": "54a9cca8-ce9a-4b81-fe76-706a230f1851"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2870/2870 [00:00<00:00, 95169.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Debug] Index -> Word vocabulary size: 9819\n",
            "[Debug] Word -> Index vocabulary size: 9819\n",
            "[Debug] Some words: [('write', 1), ('a', 2), ('uni', 3), ('essay', 4), ('in', 5), ('my', 6), ('local', 7), ('pub', 8), ('with', 9), ('coffee', 10)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vocabulary Evaluation"
      ],
      "metadata": {
        "id": "PsjJBA41okTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_vocabulary(idx_to_word: Dict[int, str], word_to_idx: Dict[str, int],\n",
        "                        word_listing: List[str], df: pd.DataFrame, check_default_size: bool = False):\n",
        "    print(\"[Vocabulary Evaluation] Size checking...\")\n",
        "    assert len(idx_to_word) == len(word_to_idx)\n",
        "    assert len(idx_to_word) == len(word_listing)\n",
        "\n",
        "    print(\"[Vocabulary Evaluation] Content checking...\")\n",
        "    for i in tqdm(range(len(idx_to_word))):\n",
        "        assert idx_to_word[i] in word_to_idx\n",
        "        assert word_to_idx[idx_to_word[i]] == i\n",
        "\n",
        "    print(\"[Vocabulary Evaluation] Consistency checking...\")\n",
        "    _, _, first_word_listing = build_vocabulary(df)\n",
        "    _, _, second_word_listing = build_vocabulary(df)\n",
        "    assert first_word_listing == second_word_listing\n",
        "\n",
        "    print(\"[Vocabulary Evaluation] Toy example checking...\")\n",
        "    toy_df = pd.DataFrame.from_dict({\n",
        "        'tweet': [\"all that glitters is not gold\", \"all in all i like this assignment\"]\n",
        "    })\n",
        "    _, _, toy_word_listing = build_vocabulary(toy_df)\n",
        "    toy_valid_vocabulary = set(' '.join(toy_df.tweet.values).split())\n",
        "    # Includi anche [UNK] nel confronto\n",
        "    toy_valid_vocabulary.add(\"[UNK]\")\n",
        "    assert set(toy_word_listing) == toy_valid_vocabulary"
      ],
      "metadata": {
        "id": "UqvEPsy8ocTT"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vocabulary evaluation...\")\n",
        "evaluate_vocabulary(idx_to_word_train, word_to_idx_train, word_listing_train, df_train)\n",
        "print(\"Evaluation completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hb7Eh1gooTu",
        "outputId": "3afcabfc-50b5-4d3f-dd3b-362a0c05c102"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary evaluation...\n",
            "[Vocabulary Evaluation] Size checking...\n",
            "[Vocabulary Evaluation] Content checking...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9819/9819 [00:00<00:00, 1678165.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Vocabulary Evaluation] Consistency checking...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2870/2870 [00:00<00:00, 173039.31it/s]\n",
            "100%|██████████| 2870/2870 [00:00<00:00, 153312.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Vocabulary Evaluation] Toy example checking...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 3509.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation completed!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving Vocabulary"
      ],
      "metadata": {
        "id": "rYzT9t9Gqj3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_path = Path.cwd().joinpath('Datasets', 'vocab.json')\n",
        "\n",
        "print(f\"Saving vocabulary to {vocab_path}\")\n",
        "with vocab_path.open(mode='w') as f:\n",
        "    sj.dump(word_to_idx_train, f, indent=4)\n",
        "print(\"Saving completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVMJ5eRtqv0N",
        "outputId": "48e7e41d-e41a-4098-c341-a01c9fb56b95"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving vocabulary to /content/Datasets/vocab.json\n",
            "Saving completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GloVe embedding"
      ],
      "metadata": {
        "id": "5MS28pC08ztV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_embedding_model(model_type: str,\n",
        "                         embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
        "    \"\"\"\n",
        "    Loads a pre-trained word embedding model via gensim library.\n",
        "\n",
        "    :param model_type: name of the word embedding model to load.\n",
        "    :param embedding_dimension: size of the embedding space to consider\n",
        "\n",
        "    :return\n",
        "        - pre-trained word embedding model (gensim KeyedVectors object)\n",
        "    \"\"\"\n",
        "    download_path = \"\"\n",
        "\n",
        "    if model_type.strip().lower() == 'glove':\n",
        "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
        "\n",
        "    else:\n",
        "        raise AttributeError(\"Unsupported embedding model type! Available ones: word2vec, glove, fasttext\")\n",
        "\n",
        "    try:\n",
        "        emb_model = gloader.load(download_path)\n",
        "    except ValueError as e:\n",
        "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
        "        print(\"Glove: 50, 100, 200, 300\")\n",
        "        raise e\n",
        "\n",
        "    return emb_model"
      ],
      "metadata": {
        "id": "fhIaF3QG8_Tf"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = load_embedding_model(model_type=\"glove\",\n",
        "                                       embedding_dimension=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kT3IwEKQ9JSU",
        "outputId": "b75b2a75-3bd4-448d-9b83-2a18570fd08e"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_OOV_terms(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
        "                    word_listing: List[str]):\n",
        "    \"\"\"\n",
        "    Checks differences between pre-trained embedding model vocabulary\n",
        "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param word_listing: dataset specific vocabulary (list)\n",
        "\n",
        "    :return\n",
        "        - list of OOV terms\n",
        "    \"\"\"\n",
        "    embedding_vocabulary = set(embedding_model.key_to_index.keys())\n",
        "    oov = set(word_listing).difference(embedding_vocabulary)\n",
        "    return list(oov)"
      ],
      "metadata": {
        "id": "gbxw6ndV_NQf"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oov_terms_train = check_OOV_terms(embedding_model, word_listing_train)\n",
        "oov_percentage_train = float(len(oov_terms_train)) * 100 / len(word_listing_train)\n",
        "print(f\"Total OOV terms in training set: {len(oov_terms_train)} ({oov_percentage_train:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W783gsHF_QD4",
        "outputId": "b37e49dd-5023-40e9-c541-8d487c3bdec2"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total OOV terms in training set: 1841 (18.75%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to use Blob correct in order to correct spelling errors in tweets."
      ],
      "metadata": {
        "id": "6RKUpyOzy4-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling train dataset OOV terms"
      ],
      "metadata": {
        "id": "7jWTz4quZxHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to put all the unseen tokens in the train dataset in our vocabulary."
      ],
      "metadata": {
        "id": "-EFIJbpwZ9DA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_embedding_matrix(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
        "                           embedding_dimension: int,\n",
        "                           word_to_idx: Dict[str, int],\n",
        "                           vocab_size: int,\n",
        "                           oov_terms: List[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
        "\n",
        "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
        "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
        "    :param vocab_size: size of the vocabulary\n",
        "    :param oov_terms: list of OOV terms (list)\n",
        "\n",
        "    :return\n",
        "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
        "    \"\"\"\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dimension), dtype=np.float32)\n",
        "    for word, idx in tqdm(word_to_idx.items()):\n",
        "        try:\n",
        "            embedding_vector = embedding_model[word]\n",
        "        except (KeyError, TypeError):\n",
        "          if word == '[UNK]':\n",
        "            # we assign a random embedding to the [UNK] token\n",
        "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
        "          else:\n",
        "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
        "\n",
        "        embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "    return embedding_matrix"
      ],
      "metadata": {
        "id": "g5n39svJA2SZ"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "embedding_dimension = 50\n",
        "embedding_matrix = build_embedding_matrix(embedding_model, embedding_dimension, word_to_idx_train, len(word_to_idx_train), oov_terms_train)\n",
        "print(f\"\\nEmbedding matrix shape: {embedding_matrix.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQcy1MNsA4mi",
        "outputId": "e62ce2be-d645-4965-c0ae-7458af193650"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9819/9819 [00:00<00:00, 284475.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding matrix shape: (9819, 50)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}