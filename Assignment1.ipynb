{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "sz13BvtmN49a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "C8iazcwMNM8Q"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import shutil\n",
        "import json\n",
        "import urllib\n",
        "import tarfile\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import re\n",
        "import emoji\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "from typing import Iterable\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Corpus"
      ],
      "metadata": {
        "id": "aorcZzjLOQII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Downloading the dataset"
      ],
      "metadata": {
        "id": "S7xVZIPFOXPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all we need to **download** the `A1/data` folder."
      ],
      "metadata": {
        "id": "vz0BiOVBOhui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DownloadProgressBar(tqdm):\n",
        "    def update_to(self, b=1, bsize=1, tsize=None):\n",
        "        if tsize is not None:\n",
        "            self.total = tsize\n",
        "        self.update(b * bsize - self.n)\n",
        "\n",
        "def download_url(download_path: Path, url: str):\n",
        "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
        "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
        "        urllib.request.urlretrieve(url, filename=download_path, reporthook=t.update_to)"
      ],
      "metadata": {
        "id": "5CFbK72BOtVu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_dataset(download_path: Path, url: str):\n",
        "    print(\"Downloading dataset...\")\n",
        "    download_url(url=url, download_path=download_path)\n",
        "    print(\"Download complete!\")"
      ],
      "metadata": {
        "id": "4Jwr7Ns6Ot1i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we put all the urls\n",
        "urls = {\n",
        "    \"training\": \"https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/main/2024-2025/Assignment%201/data/training.json\",\n",
        "    \"test\": \"https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/main/2024-2025/Assignment%201/data/test.json\",\n",
        "    \"validation\": \"https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/main/2024-2025/Assignment%201/data/validation.json\"\n",
        "}"
      ],
      "metadata": {
        "id": "jsZv2RSOViC8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Current work directory: {Path.cwd()}\")\n",
        "dataset_folder = Path.cwd().joinpath(\"Datasets\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NvOZ5g2O8Ra",
        "outputId": "f7611cb9-97af-4c28-e5cf-836e6369d92c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current work directory: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not dataset_folder.exists():\n",
        "    dataset_folder.mkdir(parents=True)"
      ],
      "metadata": {
        "id": "mGQb4TxtRYYm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, url in urls.items():\n",
        "    download_path = dataset_folder.joinpath(f\"{name}.json\")\n",
        "    download_dataset(download_path, url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r99L1rTzV12C",
        "outputId": "dbdd8529-fb5b-4a0c-90cc-bbf1a0e8a218"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training.json: 6.23MB [00:00, 109MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete!\n",
            "Downloading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test.json: 500kB [00:00, 26.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete!\n",
            "Downloading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "validation.json: 1.16MB [00:00, 58.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load the three JSON files and encode them as pandas dataframes."
      ],
      "metadata": {
        "id": "97p0MGpIYdGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_json_file(file_path: Path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)"
      ],
      "metadata": {
        "id": "tjPonkkIduMr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe_rows = []\n",
        "\n",
        "\n",
        "for name, url in urls.items():\n",
        "    # per ogni file creiamo il file_path e leggiamo il file\n",
        "    file_path = dataset_folder.joinpath(f\"{name}.json\")\n",
        "\n",
        "    json_data = load_json_file(file_path)\n",
        "\n",
        "    # per ogni chiave nel json_data creo una dataframe_row\n",
        "    for key in json_data.keys():\n",
        "        df_row = json_data[key]\n",
        "        df_row[\"split\"] = name\n",
        "        dataframe_rows.append(df_row)\n",
        ""
      ],
      "metadata": {
        "id": "mUz9IDNUcb7J"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder = Path.cwd().joinpath(\"Datasets\", \"Dataframes\")\n",
        "if not folder.exists():\n",
        "    folder.mkdir(parents=True)\n",
        "\n",
        "\n",
        "# transform the list of rows in a proper dataframe\n",
        "df = pd.DataFrame(dataframe_rows)\n",
        "\n",
        "for name, url in urls.items():\n",
        "  df_path = folder.with_name(name + \".pkl\")\n",
        "  df.to_pickle(df_path)"
      ],
      "metadata": {
        "id": "u2xd6biKiKbW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sq63x0CuijnO",
        "outputId": "9d01002b-c72a-4240-c89f-6887eb6dbbad"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     id_EXIST lang                                              tweet  \\\n",
            "0      100001   es  @TheChiflis Ignora al otro, es un capullo.El p...   \n",
            "1      100002   es  @ultimonomada_ Si comicsgate se parece en algo...   \n",
            "2      100003   es  @Steven2897 Lee sobre Gamergate, y como eso ha...   \n",
            "3      100004   es  @Lunariita7 Un retraso social bastante lamenta...   \n",
            "4      100005   es  @novadragon21 @icep4ck @TvDannyZ Entonces como...   \n",
            "...       ...  ...                                                ...   \n",
            "7953   400173   en  Amazing that the GOP is trying to take away ou...   \n",
            "7954   400174   en  It is is impossible for a man to become a woma...   \n",
            "7955   400175   en  If Gaga decided to sing 18 versions of Free Wo...   \n",
            "7956   400176   en  This is your reminder that you can be child-fr...   \n",
            "7957   400177   en  just completed my last final, iâ€™m officially a...   \n",
            "\n",
            "      number_annotators                                         annotators  \\\n",
            "0                     6  [Annotator_1, Annotator_2, Annotator_3, Annota...   \n",
            "1                     6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
            "2                     6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
            "3                     6  [Annotator_13, Annotator_14, Annotator_15, Ann...   \n",
            "4                     6  [Annotator_19, Annotator_20, Annotator_21, Ann...   \n",
            "...                 ...                                                ...   \n",
            "7953                  6  [Annotator_805, Annotator_426, Annotator_806, ...   \n",
            "7954                  6  [Annotator_770, Annotator_771, Annotator_772, ...   \n",
            "7955                  6  [Annotator_764, Annotator_765, Annotator_766, ...   \n",
            "7956                  6  [Annotator_795, Annotator_796, Annotator_797, ...   \n",
            "7957                  6  [Annotator_770, Annotator_771, Annotator_772, ...   \n",
            "\n",
            "       gender_annotators                          age_annotators  \\\n",
            "0     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
            "1     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
            "2     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
            "3     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
            "4     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
            "...                  ...                                     ...   \n",
            "7953  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
            "7954  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
            "7955  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
            "7956  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
            "7957  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
            "\n",
            "                       labels_task1  \\\n",
            "0     [YES, YES, NO, YES, YES, YES]   \n",
            "1         [NO, NO, NO, NO, YES, NO]   \n",
            "2          [NO, NO, NO, NO, NO, NO]   \n",
            "3       [NO, NO, YES, NO, YES, YES]   \n",
            "4      [YES, NO, YES, NO, YES, YES]   \n",
            "...                             ...   \n",
            "7953    [YES, YES, NO, NO, YES, NO]   \n",
            "7954   [YES, YES, YES, YES, NO, NO]   \n",
            "7955      [NO, NO, NO, NO, NO, YES]   \n",
            "7956     [NO, YES, YES, NO, NO, NO]   \n",
            "7957       [NO, NO, NO, NO, NO, NO]   \n",
            "\n",
            "                                           labels_task2  \\\n",
            "0     [REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...   \n",
            "1                               [-, -, -, -, DIRECT, -]   \n",
            "2                                    [-, -, -, -, -, -]   \n",
            "3                 [-, -, DIRECT, -, REPORTED, REPORTED]   \n",
            "4     [REPORTED, -, JUDGEMENTAL, -, JUDGEMENTAL, DIR...   \n",
            "...                                                 ...   \n",
            "7953      [REPORTED, JUDGEMENTAL, -, -, JUDGEMENTAL, -]   \n",
            "7954        [DIRECT, JUDGEMENTAL, DIRECT, DIRECT, -, -]   \n",
            "7955                            [-, -, -, -, -, DIRECT]   \n",
            "7956                [-, REPORTED, JUDGEMENTAL, -, -, -]   \n",
            "7957                                 [-, -, -, -, -, -]   \n",
            "\n",
            "                                           labels_task3       split  \n",
            "0     [[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...    training  \n",
            "1          [[-], [-], [-], [-], [OBJECTIFICATION], [-]]    training  \n",
            "2                        [[-], [-], [-], [-], [-], [-]]    training  \n",
            "3     [[-], [-], [IDEOLOGICAL-INEQUALITY], [-], [IDE...    training  \n",
            "4     [[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [-...    training  \n",
            "...                                                 ...         ...  \n",
            "7953  [[IDEOLOGICAL-INEQUALITY], [IDEOLOGICAL-INEQUA...  validation  \n",
            "7954  [[IDEOLOGICAL-INEQUALITY], [IDEOLOGICAL-INEQUA...  validation  \n",
            "7955       [[-], [-], [-], [-], [-], [SEXUAL-VIOLENCE]]  validation  \n",
            "7956  [[-], [STEREOTYPING-DOMINANCE], [IDEOLOGICAL-I...  validation  \n",
            "7957                     [[-], [-], [-], [-], [-], [-]]  validation  \n",
            "\n",
            "[7958 rows x 11 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Generate hard labels"
      ],
      "metadata": {
        "id": "aegG34haxrJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate hard labels for Task 1 using majority voting and store them in a new dataframe column called `hard_label_task1`. Items without a clear majority will be removed from the dataset."
      ],
      "metadata": {
        "id": "c6VZjWS-xvb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_majority_voting(labels: list):\n",
        "\n",
        "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
        "    majority_label = np.argwhere(counts == np.max(counts))\n",
        "\n",
        "    majority_label = unique_labels[majority_label].flatten().tolist()\n",
        "\n",
        "    if len(majority_label) > 1:\n",
        "        majority_label = None\n",
        "\n",
        "\n",
        "    return majority_label"
      ],
      "metadata": {
        "id": "sORAVONvpU_v"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_hard_labels(df):\n",
        "    hard_labels = []\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        # Estrai le etichette dalla colonna 'labels_task1'\n",
        "        labels = row['labels_task1']\n",
        "        # print(labels)\n",
        "\n",
        "        # Verifica se 'labels' Ã¨ una lista e contiene elementi\n",
        "        if isinstance(labels, list) and len(labels) > 0:\n",
        "            # Calcola la moda (voto di maggioranza)\n",
        "            most_common_label = compute_majority_voting(labels)\n",
        "            # print(most_common_label)\n",
        "            hard_labels.append(most_common_label)\n",
        "\n",
        "    # Aggiungi le hard labels come nuova colonna\n",
        "    df['hard_label_task1'] = hard_labels\n",
        "\n",
        "    # Rimuovi le righe senza una chiara maggioranza (se necessario)\n",
        "    df = df[df['hard_label_task1'].notnull()]\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "0LP9rRC1zG0Q"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = generate_hard_labels(df)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckDgdopDz3d2",
        "outputId": "018295d4-652e-4770-b17b-a73884d900a8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     id_EXIST lang                                              tweet  \\\n",
            "0      100001   es  @TheChiflis Ignora al otro, es un capullo.El p...   \n",
            "1      100002   es  @ultimonomada_ Si comicsgate se parece en algo...   \n",
            "2      100003   es  @Steven2897 Lee sobre Gamergate, y como eso ha...   \n",
            "4      100005   es  @novadragon21 @icep4ck @TvDannyZ Entonces como...   \n",
            "5      100006   es  @yonkykong Aaah sÃ­. Andrew Dobson. El que se d...   \n",
            "...       ...  ...                                                ...   \n",
            "7952   400172   en  @leesu44 @elishabroadway @markbann57 @SeaeyesT...   \n",
            "7954   400174   en  It is is impossible for a man to become a woma...   \n",
            "7955   400175   en  If Gaga decided to sing 18 versions of Free Wo...   \n",
            "7956   400176   en  This is your reminder that you can be child-fr...   \n",
            "7957   400177   en  just completed my last final, iâ€™m officially a...   \n",
            "\n",
            "      number_annotators                                         annotators  \\\n",
            "0                     6  [Annotator_1, Annotator_2, Annotator_3, Annota...   \n",
            "1                     6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
            "2                     6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
            "4                     6  [Annotator_19, Annotator_20, Annotator_21, Ann...   \n",
            "5                     6  [Annotator_25, Annotator_26, Annotator_27, Ann...   \n",
            "...                 ...                                                ...   \n",
            "7952                  6  [Annotator_780, Annotator_781, Annotator_782, ...   \n",
            "7954                  6  [Annotator_770, Annotator_771, Annotator_772, ...   \n",
            "7955                  6  [Annotator_764, Annotator_765, Annotator_766, ...   \n",
            "7956                  6  [Annotator_795, Annotator_796, Annotator_797, ...   \n",
            "7957                  6  [Annotator_770, Annotator_771, Annotator_772, ...   \n",
            "\n",
            "       gender_annotators                          age_annotators  \\\n",
            "0     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
            "1     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
            "2     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
            "4     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
            "5     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
            "...                  ...                                     ...   \n",
            "7952  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
            "7954  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
            "7955  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
            "7956  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
            "7957  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
            "\n",
            "                       labels_task1  \\\n",
            "0     [YES, YES, NO, YES, YES, YES]   \n",
            "1         [NO, NO, NO, NO, YES, NO]   \n",
            "2          [NO, NO, NO, NO, NO, NO]   \n",
            "4      [YES, NO, YES, NO, YES, YES]   \n",
            "5          [NO, NO, NO, NO, NO, NO]   \n",
            "...                             ...   \n",
            "7952  [YES, YES, NO, YES, YES, YES]   \n",
            "7954   [YES, YES, YES, YES, NO, NO]   \n",
            "7955      [NO, NO, NO, NO, NO, YES]   \n",
            "7956     [NO, YES, YES, NO, NO, NO]   \n",
            "7957       [NO, NO, NO, NO, NO, NO]   \n",
            "\n",
            "                                           labels_task2  \\\n",
            "0     [REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...   \n",
            "1                               [-, -, -, -, DIRECT, -]   \n",
            "2                                    [-, -, -, -, -, -]   \n",
            "4     [REPORTED, -, JUDGEMENTAL, -, JUDGEMENTAL, DIR...   \n",
            "5                                    [-, -, -, -, -, -]   \n",
            "...                                                 ...   \n",
            "7952  [DIRECT, REPORTED, -, JUDGEMENTAL, DIRECT, JUD...   \n",
            "7954        [DIRECT, JUDGEMENTAL, DIRECT, DIRECT, -, -]   \n",
            "7955                            [-, -, -, -, -, DIRECT]   \n",
            "7956                [-, REPORTED, JUDGEMENTAL, -, -, -]   \n",
            "7957                                 [-, -, -, -, -, -]   \n",
            "\n",
            "                                           labels_task3       split  \\\n",
            "0     [[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...    training   \n",
            "1          [[-], [-], [-], [-], [OBJECTIFICATION], [-]]    training   \n",
            "2                        [[-], [-], [-], [-], [-], [-]]    training   \n",
            "4     [[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [-...    training   \n",
            "5                        [[-], [-], [-], [-], [-], [-]]    training   \n",
            "...                                                 ...         ...   \n",
            "7952  [[IDEOLOGICAL-INEQUALITY, STEREOTYPING-DOMINAN...  validation   \n",
            "7954  [[IDEOLOGICAL-INEQUALITY], [IDEOLOGICAL-INEQUA...  validation   \n",
            "7955       [[-], [-], [-], [-], [-], [SEXUAL-VIOLENCE]]  validation   \n",
            "7956  [[-], [STEREOTYPING-DOMINANCE], [IDEOLOGICAL-I...  validation   \n",
            "7957                     [[-], [-], [-], [-], [-], [-]]  validation   \n",
            "\n",
            "     hard_label_task1  \n",
            "0               [YES]  \n",
            "1                [NO]  \n",
            "2                [NO]  \n",
            "4               [YES]  \n",
            "5                [NO]  \n",
            "...               ...  \n",
            "7952            [YES]  \n",
            "7954            [YES]  \n",
            "7955             [NO]  \n",
            "7956             [NO]  \n",
            "7957             [NO]  \n",
            "\n",
            "[6998 rows x 12 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Filter the DataFrame"
      ],
      "metadata": {
        "id": "-VdYgdgwqsNu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter the DataFrame to keep only rows where the `lang` column is `'en'`."
      ],
      "metadata": {
        "id": "QVfoZzusqy0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['lang'] == 'en']\n",
        "print(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WYAWKilq1tl",
        "outputId": "c1ed2ad9-2b5b-4785-e8db-36f672cc7e26"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3314, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Remove unwanted columns"
      ],
      "metadata": {
        "id": "etQQV0YTrN9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keep only `id_EXIST`, `lang`, `tweet`, and `hard_label_task1`."
      ],
      "metadata": {
        "id": "T4nDBRNurTT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_unwanted_columns(df):\n",
        "\n",
        "    columns_to_keep = ['id_EXIST', 'lang', 'tweet', 'hard_label_task1']\n",
        "    df = df[columns_to_keep]\n",
        "    return df"
      ],
      "metadata": {
        "id": "hb0-lUqPrZEH"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = remove_unwanted_columns(df)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoZB0G8xrgcA",
        "outputId": "63da9a97-c406-4f33-c8fe-631abb117e1b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     id_EXIST lang                                              tweet  \\\n",
            "3661   200002   en  Writing a uni essay in my local pub with a cof...   \n",
            "3662   200003   en  @UniversalORL it is 2021 not 1921. I dont appr...   \n",
            "3665   200006   en  According to a customer I have plenty of time ...   \n",
            "3666   200007   en  So only 'blokes' drink beer? Sorry, but if you...   \n",
            "3667   200008   en  New to the shelves this week - looking forward...   \n",
            "...       ...  ...                                                ...   \n",
            "7952   400172   en  @leesu44 @elishabroadway @markbann57 @SeaeyesT...   \n",
            "7954   400174   en  It is is impossible for a man to become a woma...   \n",
            "7955   400175   en  If Gaga decided to sing 18 versions of Free Wo...   \n",
            "7956   400176   en  This is your reminder that you can be child-fr...   \n",
            "7957   400177   en  just completed my last final, iâ€™m officially a...   \n",
            "\n",
            "     hard_label_task1  \n",
            "3661            [YES]  \n",
            "3662            [YES]  \n",
            "3665            [YES]  \n",
            "3666            [YES]  \n",
            "3667             [NO]  \n",
            "...               ...  \n",
            "7952            [YES]  \n",
            "7954            [YES]  \n",
            "7955             [NO]  \n",
            "7956             [NO]  \n",
            "7957             [NO]  \n",
            "\n",
            "[3314 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Encode the hard_label_task1 column"
      ],
      "metadata": {
        "id": "68p9vTuLru-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use 1 to represent \"YES\" and 0 to represent \"NO\" in the `hard_label_task1 column`."
      ],
      "metadata": {
        "id": "70CeoJ1hr-rB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['hard_label_task1'] = df['hard_label_task1'].apply(lambda x: 1 if x[0] == 'YES' else 0)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lh8WC291tNGA",
        "outputId": "bbbbd9ee-3296-45a8-bfe8-88f8aed73b8a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     id_EXIST lang                                              tweet  \\\n",
            "3661   200002   en  Writing a uni essay in my local pub with a cof...   \n",
            "3662   200003   en  @UniversalORL it is 2021 not 1921. I dont appr...   \n",
            "3665   200006   en  According to a customer I have plenty of time ...   \n",
            "3666   200007   en  So only 'blokes' drink beer? Sorry, but if you...   \n",
            "3667   200008   en  New to the shelves this week - looking forward...   \n",
            "...       ...  ...                                                ...   \n",
            "7952   400172   en  @leesu44 @elishabroadway @markbann57 @SeaeyesT...   \n",
            "7954   400174   en  It is is impossible for a man to become a woma...   \n",
            "7955   400175   en  If Gaga decided to sing 18 versions of Free Wo...   \n",
            "7956   400176   en  This is your reminder that you can be child-fr...   \n",
            "7957   400177   en  just completed my last final, iâ€™m officially a...   \n",
            "\n",
            "      hard_label_task1  \n",
            "3661                 1  \n",
            "3662                 1  \n",
            "3665                 1  \n",
            "3666                 1  \n",
            "3667                 0  \n",
            "...                ...  \n",
            "7952                 1  \n",
            "7954                 1  \n",
            "7955                 0  \n",
            "7956                 0  \n",
            "7957                 0  \n",
            "\n",
            "[3314 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Data Cleaning"
      ],
      "metadata": {
        "id": "qyAK2RBGu8Bq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9rsrT-WJDoK",
        "outputId": "c61072e1-09ea-4451-f897-391e4fc13a31"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet = \"Check out my blog at http://example.com! ðŸ˜Š #CodingIsFun @user123 'It's a great day!' ðŸŽ‰ Let's celebrate! #Python #100DaysOfCode. I am running in the park and enjoying the beautiful days with my friends. We are having so much fun! Let's make memories! #FunTimes #Friendship\""
      ],
      "metadata": {
        "id": "r2d-wEci2J6L"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check this link: [all existing emojis](https://www.unicode.org/Public/emoji/1.0//emoji-data.txt). And also this: [emojis unicode consortium](https://unicode.org/emoji/charts/full-emoji-list.html)."
      ],
      "metadata": {
        "id": "W3yM7nuc_DpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_emoji(text):\n",
        "    return emoji.replace_emoji(text, replace='')"
      ],
      "metadata": {
        "id": "2U3Eg7_4_-Ic"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_hastag(text):\n",
        "    at = re.compile(r'#\\S+')\n",
        "    return at.sub(r'',text)"
      ],
      "metadata": {
        "id": "69qq6Fe25xMc"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_mention(text):\n",
        "    at = re.compile(r'@\\S+')\n",
        "    return at.sub(r'',text)"
      ],
      "metadata": {
        "id": "rp8HhWlh5NEu"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_URL(text):\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url.sub(r'',text)"
      ],
      "metadata": {
        "id": "GwVvkIsa64wk"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_special_characters(text):\n",
        "    pattern = r'[^a-zA-Z0-9\\s]'\n",
        "    return re.sub(pattern, '', text)"
      ],
      "metadata": {
        "id": "iLRDqOqS7QKt"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_quotes(text):\n",
        "    pattern = r'^\"|\"$â€˜â€™'\n",
        "    return re.sub(pattern, '', text)"
      ],
      "metadata": {
        "id": "v__lt8yu8Oap"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_extra_spaces(text):\n",
        "    pattern = r'\\s+'\n",
        "    return re.sub(pattern, ' ', text)"
      ],
      "metadata": {
        "id": "6aRkZPTP9KG0"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "    def get_wordnet_key(pos_tag):\n",
        "        if pos_tag.startswith('J'):\n",
        "            return wordnet.ADJ\n",
        "        elif pos_tag.startswith('V'):\n",
        "            return wordnet.VERB\n",
        "        elif pos_tag.startswith('N'):\n",
        "            return wordnet.NOUN\n",
        "        elif pos_tag.startswith('R'):\n",
        "            return wordnet.ADV\n",
        "        else:\n",
        "            return 'n'\n",
        "\n",
        "\n",
        "    def lem_text(text: str):\n",
        "        tokens = nltk.word_tokenize(text)  # Assicurati di importare il tokenizer\n",
        "        tagged = pos_tag(tokens)\n",
        "        words = [lemmatizer.lemmatize(word, get_wordnet_key(tag)) for word, tag in tagged]\n",
        "        return \" \".join(words)\n",
        "\n",
        "\n",
        "    return lem_text(text)"
      ],
      "metadata": {
        "id": "Eb2ni1lLI4oH"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tweet)\n",
        "\n",
        "tweet = remove_emoji(tweet)\n",
        "print(f\"No emojis: {tweet}\\n\")\n",
        "\n",
        "tweet = remove_hastag(tweet)\n",
        "print(f\"No hastag: {tweet}\")\n",
        "\n",
        "tweet = remove_mention(tweet)\n",
        "print(f\"No mentions: {tweet}\")\n",
        "\n",
        "tweet = remove_URL(tweet)\n",
        "print(f\"No HTML: {tweet}\")\n",
        "\n",
        "tweet = remove_special_characters(tweet)\n",
        "print(f\"No special characters: {tweet}\")\n",
        "\n",
        "tweet = remove_quotes(tweet)\n",
        "print(f\"No quotes: {tweet}\")\n",
        "\n",
        "tweet = remove_extra_spaces(tweet)\n",
        "print(f\"No extra spaces: {tweet}\")\n",
        "\n",
        "tweet = lemmatize(tweet)\n",
        "print(f\"Lemmatized: {tweet}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ird4iOT23IEB",
        "outputId": "c52b2027-3031-4a39-c246-1ffc88a4be6c"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check out my blog at http://example.com! ðŸ˜Š #CodingIsFun @user123 'It's a great day!' ðŸŽ‰ Let's celebrate! #Python #100DaysOfCode. I am running in the park and enjoying the beautiful days with my friends. We are having so much fun! Let's make memories! #FunTimes #Friendship\n",
            "No emojis: Check out my blog at http://example.com!  #CodingIsFun @user123 'It's a great day!'  Let's celebrate! #Python #100DaysOfCode. I am running in the park and enjoying the beautiful days with my friends. We are having so much fun! Let's make memories! #FunTimes #Friendship\n",
            "\n",
            "No hastag: Check out my blog at http://example.com!   @user123 'It's a great day!'  Let's celebrate!   I am running in the park and enjoying the beautiful days with my friends. We are having so much fun! Let's make memories!  \n",
            "No mentions: Check out my blog at http://example.com!    'It's a great day!'  Let's celebrate!   I am running in the park and enjoying the beautiful days with my friends. We are having so much fun! Let's make memories!  \n",
            "No HTML: Check out my blog at     'It's a great day!'  Let's celebrate!   I am running in the park and enjoying the beautiful days with my friends. We are having so much fun! Let's make memories!  \n",
            "No special characters: Check out my blog at     Its a great day  Lets celebrate   I am running in the park and enjoying the beautiful days with my friends We are having so much fun Lets make memories  \n",
            "No quotes: Check out my blog at     Its a great day  Lets celebrate   I am running in the park and enjoying the beautiful days with my friends We are having so much fun Lets make memories  \n",
            "No extra spaces: Check out my blog at Its a great day Lets celebrate I am running in the park and enjoying the beautiful days with my friends We are having so much fun Lets make memories \n",
            "Lemmatized: Check out my blog at Its a great day Lets celebrate I be run in the park and enjoy the beautiful day with my friend We be have so much fun Lets make memory\n"
          ]
        }
      ]
    }
  ]
}