{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "sz13BvtmN49a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n",
        "!pip install nltk\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WE9zjHoMn6h",
        "outputId": "d072bc21-1543-47eb-9e10-9ae1fd8ec993"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/586.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.9/586.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "C8iazcwMNM8Q"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import shutil\n",
        "import json\n",
        "import urllib\n",
        "import tarfile\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import re\n",
        "import emoji\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "from typing import Iterable\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Corpus"
      ],
      "metadata": {
        "id": "aorcZzjLOQII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Downloading the dataset"
      ],
      "metadata": {
        "id": "S7xVZIPFOXPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all we need to **download** the `A1/data` folder."
      ],
      "metadata": {
        "id": "vz0BiOVBOhui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DownloadProgressBar(tqdm):\n",
        "    def update_to(self, b=1, bsize=1, tsize=None):\n",
        "        if tsize is not None:\n",
        "            self.total = tsize\n",
        "        self.update(b * bsize - self.n)\n",
        "\n",
        "def download_url(download_path: Path, url: str):\n",
        "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
        "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
        "        urllib.request.urlretrieve(url, filename=download_path, reporthook=t.update_to)"
      ],
      "metadata": {
        "id": "5CFbK72BOtVu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_dataset(download_path: Path, url: str):\n",
        "    print(\"Downloading dataset...\")\n",
        "    download_url(url=url, download_path=download_path)\n",
        "    print(\"Download complete!\")"
      ],
      "metadata": {
        "id": "4Jwr7Ns6Ot1i"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we put all the urls\n",
        "urls = {\n",
        "    \"training\": \"https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/main/2024-2025/Assignment%201/data/training.json\",\n",
        "    \"test\": \"https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/main/2024-2025/Assignment%201/data/test.json\",\n",
        "    \"validation\": \"https://raw.githubusercontent.com/nlp-unibo/nlp-course-material/main/2024-2025/Assignment%201/data/validation.json\"\n",
        "}"
      ],
      "metadata": {
        "id": "jsZv2RSOViC8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Current work directory: {Path.cwd()}\")\n",
        "dataset_folder = Path.cwd().joinpath(\"Datasets\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NvOZ5g2O8Ra",
        "outputId": "47a0d567-17a1-46e5-97c3-b641d12d6cdd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current work directory: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not dataset_folder.exists():\n",
        "    dataset_folder.mkdir(parents=True)"
      ],
      "metadata": {
        "id": "mGQb4TxtRYYm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, url in urls.items():\n",
        "    download_path = dataset_folder.joinpath(f\"{name}.json\")\n",
        "    download_dataset(download_path, url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r99L1rTzV12C",
        "outputId": "ae87e91e-b5af-431a-d5ac-99be0bc702d3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training.json: 6.23MB [00:00, 14.3MB/s]                            \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete!\n",
            "Downloading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test.json: 500kB [00:00, 2.11MB/s]                            \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete!\n",
            "Downloading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "validation.json: 1.16MB [00:00, 4.22MB/s]                            "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load the three JSON files and encode them as pandas dataframes."
      ],
      "metadata": {
        "id": "97p0MGpIYdGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_json_file(file_path: Path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)"
      ],
      "metadata": {
        "id": "tjPonkkIduMr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe_rows = []\n",
        "\n",
        "\n",
        "for name, url in urls.items():\n",
        "    # per ogni file creiamo il file_path e leggiamo il file\n",
        "    file_path = dataset_folder.joinpath(f\"{name}.json\")\n",
        "\n",
        "    json_data = load_json_file(file_path)\n",
        "\n",
        "    # per ogni chiave nel json_data creo una dataframe_row\n",
        "    for key in json_data.keys():\n",
        "        df_row = json_data[key]\n",
        "        df_row[\"split\"] = name\n",
        "        dataframe_rows.append(df_row)\n",
        ""
      ],
      "metadata": {
        "id": "mUz9IDNUcb7J"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder = Path.cwd().joinpath(\"Datasets\", \"Dataframes\")\n",
        "if not folder.exists():\n",
        "    folder.mkdir(parents=True)\n",
        "\n",
        "\n",
        "# transform the list of rows in a proper dataframe\n",
        "df = pd.DataFrame(dataframe_rows)\n",
        "\n",
        "for name, url in urls.items():\n",
        "  df_path = folder.with_name(name + \".pkl\")\n",
        "  df.to_pickle(df_path)"
      ],
      "metadata": {
        "id": "u2xd6biKiKbW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sq63x0CuijnO",
        "outputId": "887a6087-e830-452d-fcf9-736c45e55296"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     id_EXIST lang                                              tweet  \\\n",
            "0      100001   es  @TheChiflis Ignora al otro, es un capullo.El p...   \n",
            "1      100002   es  @ultimonomada_ Si comicsgate se parece en algo...   \n",
            "2      100003   es  @Steven2897 Lee sobre Gamergate, y como eso ha...   \n",
            "3      100004   es  @Lunariita7 Un retraso social bastante lamenta...   \n",
            "4      100005   es  @novadragon21 @icep4ck @TvDannyZ Entonces como...   \n",
            "...       ...  ...                                                ...   \n",
            "7953   400173   en  Amazing that the GOP is trying to take away ou...   \n",
            "7954   400174   en  It is is impossible for a man to become a woma...   \n",
            "7955   400175   en  If Gaga decided to sing 18 versions of Free Wo...   \n",
            "7956   400176   en  This is your reminder that you can be child-fr...   \n",
            "7957   400177   en  just completed my last final, i’m officially a...   \n",
            "\n",
            "      number_annotators                                         annotators  \\\n",
            "0                     6  [Annotator_1, Annotator_2, Annotator_3, Annota...   \n",
            "1                     6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
            "2                     6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
            "3                     6  [Annotator_13, Annotator_14, Annotator_15, Ann...   \n",
            "4                     6  [Annotator_19, Annotator_20, Annotator_21, Ann...   \n",
            "...                 ...                                                ...   \n",
            "7953                  6  [Annotator_805, Annotator_426, Annotator_806, ...   \n",
            "7954                  6  [Annotator_770, Annotator_771, Annotator_772, ...   \n",
            "7955                  6  [Annotator_764, Annotator_765, Annotator_766, ...   \n",
            "7956                  6  [Annotator_795, Annotator_796, Annotator_797, ...   \n",
            "7957                  6  [Annotator_770, Annotator_771, Annotator_772, ...   \n",
            "\n",
            "       gender_annotators                          age_annotators  \\\n",
            "0     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
            "1     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
            "2     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
            "3     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
            "4     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
            "...                  ...                                     ...   \n",
            "7953  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
            "7954  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
            "7955  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
            "7956  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
            "7957  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
            "\n",
            "                       labels_task1  \\\n",
            "0     [YES, YES, NO, YES, YES, YES]   \n",
            "1         [NO, NO, NO, NO, YES, NO]   \n",
            "2          [NO, NO, NO, NO, NO, NO]   \n",
            "3       [NO, NO, YES, NO, YES, YES]   \n",
            "4      [YES, NO, YES, NO, YES, YES]   \n",
            "...                             ...   \n",
            "7953    [YES, YES, NO, NO, YES, NO]   \n",
            "7954   [YES, YES, YES, YES, NO, NO]   \n",
            "7955      [NO, NO, NO, NO, NO, YES]   \n",
            "7956     [NO, YES, YES, NO, NO, NO]   \n",
            "7957       [NO, NO, NO, NO, NO, NO]   \n",
            "\n",
            "                                           labels_task2  \\\n",
            "0     [REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...   \n",
            "1                               [-, -, -, -, DIRECT, -]   \n",
            "2                                    [-, -, -, -, -, -]   \n",
            "3                 [-, -, DIRECT, -, REPORTED, REPORTED]   \n",
            "4     [REPORTED, -, JUDGEMENTAL, -, JUDGEMENTAL, DIR...   \n",
            "...                                                 ...   \n",
            "7953      [REPORTED, JUDGEMENTAL, -, -, JUDGEMENTAL, -]   \n",
            "7954        [DIRECT, JUDGEMENTAL, DIRECT, DIRECT, -, -]   \n",
            "7955                            [-, -, -, -, -, DIRECT]   \n",
            "7956                [-, REPORTED, JUDGEMENTAL, -, -, -]   \n",
            "7957                                 [-, -, -, -, -, -]   \n",
            "\n",
            "                                           labels_task3       split  \n",
            "0     [[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...    training  \n",
            "1          [[-], [-], [-], [-], [OBJECTIFICATION], [-]]    training  \n",
            "2                        [[-], [-], [-], [-], [-], [-]]    training  \n",
            "3     [[-], [-], [IDEOLOGICAL-INEQUALITY], [-], [IDE...    training  \n",
            "4     [[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [-...    training  \n",
            "...                                                 ...         ...  \n",
            "7953  [[IDEOLOGICAL-INEQUALITY], [IDEOLOGICAL-INEQUA...  validation  \n",
            "7954  [[IDEOLOGICAL-INEQUALITY], [IDEOLOGICAL-INEQUA...  validation  \n",
            "7955       [[-], [-], [-], [-], [-], [SEXUAL-VIOLENCE]]  validation  \n",
            "7956  [[-], [STEREOTYPING-DOMINANCE], [IDEOLOGICAL-I...  validation  \n",
            "7957                     [[-], [-], [-], [-], [-], [-]]  validation  \n",
            "\n",
            "[7958 rows x 11 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Generate hard labels"
      ],
      "metadata": {
        "id": "aegG34haxrJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate hard labels for Task 1 using majority voting and store them in a new dataframe column called `hard_label_task1`. Items without a clear majority will be removed from the dataset."
      ],
      "metadata": {
        "id": "c6VZjWS-xvb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_majority_voting(labels: list):\n",
        "\n",
        "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
        "    majority_label = np.argwhere(counts == np.max(counts))\n",
        "\n",
        "    majority_label = unique_labels[majority_label].flatten().tolist()\n",
        "\n",
        "    if len(majority_label) > 1:\n",
        "        majority_label = None\n",
        "\n",
        "\n",
        "    return majority_label"
      ],
      "metadata": {
        "id": "sORAVONvpU_v"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_hard_labels(df):\n",
        "    hard_labels = []\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        # Estrai le etichette dalla colonna 'labels_task1'\n",
        "        labels = row['labels_task1']\n",
        "        # print(labels)\n",
        "\n",
        "        # Verifica se 'labels' è una lista e contiene elementi\n",
        "        if isinstance(labels, list) and len(labels) > 0:\n",
        "            # Calcola la moda (voto di maggioranza)\n",
        "            most_common_label = compute_majority_voting(labels)\n",
        "            # print(most_common_label)\n",
        "            hard_labels.append(most_common_label)\n",
        "\n",
        "    # Aggiungi le hard labels come nuova colonna\n",
        "    df['hard_label_task1'] = hard_labels\n",
        "\n",
        "    # Rimuovi le righe senza una chiara maggioranza (se necessario)\n",
        "    df = df[df['hard_label_task1'].notnull()]\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "0LP9rRC1zG0Q"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = generate_hard_labels(df)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckDgdopDz3d2",
        "outputId": "cd18c388-383d-4490-eeca-554a533e121a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     id_EXIST lang                                              tweet  \\\n",
            "0      100001   es  @TheChiflis Ignora al otro, es un capullo.El p...   \n",
            "1      100002   es  @ultimonomada_ Si comicsgate se parece en algo...   \n",
            "2      100003   es  @Steven2897 Lee sobre Gamergate, y como eso ha...   \n",
            "4      100005   es  @novadragon21 @icep4ck @TvDannyZ Entonces como...   \n",
            "5      100006   es  @yonkykong Aaah sí. Andrew Dobson. El que se d...   \n",
            "...       ...  ...                                                ...   \n",
            "7952   400172   en  @leesu44 @elishabroadway @markbann57 @SeaeyesT...   \n",
            "7954   400174   en  It is is impossible for a man to become a woma...   \n",
            "7955   400175   en  If Gaga decided to sing 18 versions of Free Wo...   \n",
            "7956   400176   en  This is your reminder that you can be child-fr...   \n",
            "7957   400177   en  just completed my last final, i’m officially a...   \n",
            "\n",
            "      number_annotators                                         annotators  \\\n",
            "0                     6  [Annotator_1, Annotator_2, Annotator_3, Annota...   \n",
            "1                     6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
            "2                     6  [Annotator_7, Annotator_8, Annotator_9, Annota...   \n",
            "4                     6  [Annotator_19, Annotator_20, Annotator_21, Ann...   \n",
            "5                     6  [Annotator_25, Annotator_26, Annotator_27, Ann...   \n",
            "...                 ...                                                ...   \n",
            "7952                  6  [Annotator_780, Annotator_781, Annotator_782, ...   \n",
            "7954                  6  [Annotator_770, Annotator_771, Annotator_772, ...   \n",
            "7955                  6  [Annotator_764, Annotator_765, Annotator_766, ...   \n",
            "7956                  6  [Annotator_795, Annotator_796, Annotator_797, ...   \n",
            "7957                  6  [Annotator_770, Annotator_771, Annotator_772, ...   \n",
            "\n",
            "       gender_annotators                          age_annotators  \\\n",
            "0     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
            "1     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
            "2     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
            "4     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
            "5     [F, F, F, M, M, M]  [18-22, 23-45, 46+, 46+, 23-45, 18-22]   \n",
            "...                  ...                                     ...   \n",
            "7952  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
            "7954  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
            "7955  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
            "7956  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
            "7957  [F, F, F, M, M, M]  [18-22, 23-45, 46+, 18-22, 23-45, 46+]   \n",
            "\n",
            "                       labels_task1  \\\n",
            "0     [YES, YES, NO, YES, YES, YES]   \n",
            "1         [NO, NO, NO, NO, YES, NO]   \n",
            "2          [NO, NO, NO, NO, NO, NO]   \n",
            "4      [YES, NO, YES, NO, YES, YES]   \n",
            "5          [NO, NO, NO, NO, NO, NO]   \n",
            "...                             ...   \n",
            "7952  [YES, YES, NO, YES, YES, YES]   \n",
            "7954   [YES, YES, YES, YES, NO, NO]   \n",
            "7955      [NO, NO, NO, NO, NO, YES]   \n",
            "7956     [NO, YES, YES, NO, NO, NO]   \n",
            "7957       [NO, NO, NO, NO, NO, NO]   \n",
            "\n",
            "                                           labels_task2  \\\n",
            "0     [REPORTED, JUDGEMENTAL, -, REPORTED, JUDGEMENT...   \n",
            "1                               [-, -, -, -, DIRECT, -]   \n",
            "2                                    [-, -, -, -, -, -]   \n",
            "4     [REPORTED, -, JUDGEMENTAL, -, JUDGEMENTAL, DIR...   \n",
            "5                                    [-, -, -, -, -, -]   \n",
            "...                                                 ...   \n",
            "7952  [DIRECT, REPORTED, -, JUDGEMENTAL, DIRECT, JUD...   \n",
            "7954        [DIRECT, JUDGEMENTAL, DIRECT, DIRECT, -, -]   \n",
            "7955                            [-, -, -, -, -, DIRECT]   \n",
            "7956                [-, REPORTED, JUDGEMENTAL, -, -, -]   \n",
            "7957                                 [-, -, -, -, -, -]   \n",
            "\n",
            "                                           labels_task3       split  \\\n",
            "0     [[OBJECTIFICATION], [OBJECTIFICATION, SEXUAL-V...    training   \n",
            "1          [[-], [-], [-], [-], [OBJECTIFICATION], [-]]    training   \n",
            "2                        [[-], [-], [-], [-], [-], [-]]    training   \n",
            "4     [[STEREOTYPING-DOMINANCE, OBJECTIFICATION], [-...    training   \n",
            "5                        [[-], [-], [-], [-], [-], [-]]    training   \n",
            "...                                                 ...         ...   \n",
            "7952  [[IDEOLOGICAL-INEQUALITY, STEREOTYPING-DOMINAN...  validation   \n",
            "7954  [[IDEOLOGICAL-INEQUALITY], [IDEOLOGICAL-INEQUA...  validation   \n",
            "7955       [[-], [-], [-], [-], [-], [SEXUAL-VIOLENCE]]  validation   \n",
            "7956  [[-], [STEREOTYPING-DOMINANCE], [IDEOLOGICAL-I...  validation   \n",
            "7957                     [[-], [-], [-], [-], [-], [-]]  validation   \n",
            "\n",
            "     hard_label_task1  \n",
            "0               [YES]  \n",
            "1                [NO]  \n",
            "2                [NO]  \n",
            "4               [YES]  \n",
            "5                [NO]  \n",
            "...               ...  \n",
            "7952            [YES]  \n",
            "7954            [YES]  \n",
            "7955             [NO]  \n",
            "7956             [NO]  \n",
            "7957             [NO]  \n",
            "\n",
            "[6998 rows x 12 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Filter the DataFrame"
      ],
      "metadata": {
        "id": "-VdYgdgwqsNu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter the DataFrame to keep only rows where the `lang` column is `'en'`."
      ],
      "metadata": {
        "id": "QVfoZzusqy0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['lang'] == 'en']\n",
        "print(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WYAWKilq1tl",
        "outputId": "1bd757cc-35ef-4459-a2ac-d98d40aa91bc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3314, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Remove unwanted columns"
      ],
      "metadata": {
        "id": "etQQV0YTrN9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keep only `id_EXIST`, `lang`, `tweet`, and `hard_label_task1`."
      ],
      "metadata": {
        "id": "T4nDBRNurTT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_unwanted_columns(df):\n",
        "\n",
        "    columns_to_keep = ['id_EXIST', 'lang', 'tweet', 'hard_label_task1']\n",
        "    df = df[columns_to_keep]\n",
        "    return df"
      ],
      "metadata": {
        "id": "hb0-lUqPrZEH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = remove_unwanted_columns(df)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoZB0G8xrgcA",
        "outputId": "f3f27b8b-312b-4f51-ce86-f9d69764922d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     id_EXIST lang                                              tweet  \\\n",
            "3661   200002   en  Writing a uni essay in my local pub with a cof...   \n",
            "3662   200003   en  @UniversalORL it is 2021 not 1921. I dont appr...   \n",
            "3665   200006   en  According to a customer I have plenty of time ...   \n",
            "3666   200007   en  So only 'blokes' drink beer? Sorry, but if you...   \n",
            "3667   200008   en  New to the shelves this week - looking forward...   \n",
            "...       ...  ...                                                ...   \n",
            "7952   400172   en  @leesu44 @elishabroadway @markbann57 @SeaeyesT...   \n",
            "7954   400174   en  It is is impossible for a man to become a woma...   \n",
            "7955   400175   en  If Gaga decided to sing 18 versions of Free Wo...   \n",
            "7956   400176   en  This is your reminder that you can be child-fr...   \n",
            "7957   400177   en  just completed my last final, i’m officially a...   \n",
            "\n",
            "     hard_label_task1  \n",
            "3661            [YES]  \n",
            "3662            [YES]  \n",
            "3665            [YES]  \n",
            "3666            [YES]  \n",
            "3667             [NO]  \n",
            "...               ...  \n",
            "7952            [YES]  \n",
            "7954            [YES]  \n",
            "7955             [NO]  \n",
            "7956             [NO]  \n",
            "7957             [NO]  \n",
            "\n",
            "[3314 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Encode the hard_label_task1 column"
      ],
      "metadata": {
        "id": "68p9vTuLru-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use 1 to represent \"YES\" and 0 to represent \"NO\" in the `hard_label_task1 column`."
      ],
      "metadata": {
        "id": "70CeoJ1hr-rB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['hard_label_task1'] = df['hard_label_task1'].apply(lambda x: 1 if x[0] == 'YES' else 0)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lh8WC291tNGA",
        "outputId": "bb0e8944-30de-45bd-f7e5-643f9e1e7260"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     id_EXIST lang                                              tweet  \\\n",
            "3661   200002   en  Writing a uni essay in my local pub with a cof...   \n",
            "3662   200003   en  @UniversalORL it is 2021 not 1921. I dont appr...   \n",
            "3665   200006   en  According to a customer I have plenty of time ...   \n",
            "3666   200007   en  So only 'blokes' drink beer? Sorry, but if you...   \n",
            "3667   200008   en  New to the shelves this week - looking forward...   \n",
            "...       ...  ...                                                ...   \n",
            "7952   400172   en  @leesu44 @elishabroadway @markbann57 @SeaeyesT...   \n",
            "7954   400174   en  It is is impossible for a man to become a woma...   \n",
            "7955   400175   en  If Gaga decided to sing 18 versions of Free Wo...   \n",
            "7956   400176   en  This is your reminder that you can be child-fr...   \n",
            "7957   400177   en  just completed my last final, i’m officially a...   \n",
            "\n",
            "      hard_label_task1  \n",
            "3661                 1  \n",
            "3662                 1  \n",
            "3665                 1  \n",
            "3666                 1  \n",
            "3667                 0  \n",
            "...                ...  \n",
            "7952                 1  \n",
            "7954                 1  \n",
            "7955                 0  \n",
            "7956                 0  \n",
            "7957                 0  \n",
            "\n",
            "[3314 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Data Cleaning"
      ],
      "metadata": {
        "id": "qyAK2RBGu8Bq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9rsrT-WJDoK",
        "outputId": "45d49854-23a7-4952-8e19-1b9610349aab"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check this link: [all existing emojis](https://www.unicode.org/Public/emoji/1.0//emoji-data.txt). And also this: [emojis unicode consortium](https://unicode.org/emoji/charts/full-emoji-list.html)."
      ],
      "metadata": {
        "id": "W3yM7nuc_DpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_emoji(text):\n",
        "    return emoji.replace_emoji(text, replace='')"
      ],
      "metadata": {
        "id": "2U3Eg7_4_-Ic"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_hastag(text):\n",
        "    at = re.compile(r'#\\S+')\n",
        "    return at.sub(r'',text)"
      ],
      "metadata": {
        "id": "69qq6Fe25xMc"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_mention(text):\n",
        "    at = re.compile(r'@\\S+')\n",
        "    return at.sub(r'',text)"
      ],
      "metadata": {
        "id": "rp8HhWlh5NEu"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_URL(text):\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url.sub(r'',text)"
      ],
      "metadata": {
        "id": "GwVvkIsa64wk"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_special_characters(text):\n",
        "    pattern = r'[^a-zA-Z0-9\\s]'\n",
        "    return re.sub(pattern, '', text)"
      ],
      "metadata": {
        "id": "iLRDqOqS7QKt"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_quotes(text):\n",
        "    pattern = r'^\"|\"$‘’'\n",
        "    return re.sub(pattern, '', text)"
      ],
      "metadata": {
        "id": "v__lt8yu8Oap"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_extra_spaces(text):\n",
        "    pattern = r'\\s+'\n",
        "    return re.sub(pattern, ' ', text)"
      ],
      "metadata": {
        "id": "6aRkZPTP9KG0"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "    def get_wordnet_key(pos_tag):\n",
        "        if pos_tag.startswith('J'):\n",
        "            return wordnet.ADJ\n",
        "        elif pos_tag.startswith('V'):\n",
        "            return wordnet.VERB\n",
        "        elif pos_tag.startswith('N'):\n",
        "            return wordnet.NOUN\n",
        "        elif pos_tag.startswith('R'):\n",
        "            return wordnet.ADV\n",
        "        else:\n",
        "            return 'n'\n",
        "\n",
        "\n",
        "    def lem_text(text: str):\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "        tagged = pos_tag(tokens)\n",
        "        words = [lemmatizer.lemmatize(word, get_wordnet_key(tag)) for word, tag in tagged]\n",
        "        return \" \".join(words)\n",
        "\n",
        "\n",
        "    return lem_text(text)"
      ],
      "metadata": {
        "id": "Eb2ni1lLI4oH"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_tweet(tweet: str):\n",
        "    tweet = remove_emoji(tweet)\n",
        "    tweet = remove_hastag(tweet)\n",
        "    tweet = remove_mention(tweet)\n",
        "    tweet = remove_URL(tweet)\n",
        "    tweet = remove_special_characters(tweet)\n",
        "    tweet = remove_quotes(tweet)\n",
        "    tweet = remove_extra_spaces(tweet)\n",
        "    tweet = lemmatize(tweet)\n",
        "    return tweet"
      ],
      "metadata": {
        "id": "Xid-F6PgLxoN"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['tweet'] = df['tweet'].apply(clean_tweet)"
      ],
      "metadata": {
        "id": "93acNCscMDRL"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['tweet'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcwvF65MNAXd",
        "outputId": "9abf3927-8b7f-4ea1-c84e-debe44e0016b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3661    Writing a uni essay in my local pub with a cof...\n",
            "3662    it be 2021 not 1921 I dont appreciate that on ...\n",
            "3665    According to a customer I have plenty of time ...\n",
            "3666    So only blokes drink beer Sorry but if you are...\n",
            "3667    New to the shelf this week look forward to rea...\n",
            "                              ...                        \n",
            "7952    There be even more way for a woman to prevent ...\n",
            "7954    It be be impossible for a man to become a woma...\n",
            "7955    If Gaga decide to sing 18 version of Free Woma...\n",
            "7956    This be your reminder that you can be childfre...\n",
            "7957    just complete my last final im officially a fr...\n",
            "Name: tweet, Length: 3314, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Text Encoding"
      ],
      "metadata": {
        "id": "hU2q_4tGNU2W"
      }
    }
  ]
}