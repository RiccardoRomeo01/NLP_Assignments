% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "guidelines" option to generate the final version.
%\usepackage[guidelines]{nlpreport} % show guidelines  <---------------------
\usepackage[]{nlpreport} % hide guidelines


% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs} % for tables






% THE pdfinfo Title AND Author ARE NOT NECESSARY, THEY ARE METADATA FOR THE FINAL PDF FILE
\hypersetup{pdfinfo={
Title={Guidelines and template for NLP coursework report},
Author={Jane Smith \& John Doe}
}}
%\setcounter{secnumdepth}{0}  
 \begin{document}
%
\title{Assignment 1\\
\explanation{\rm Substitute the $\uparrow$ title $\uparrow$ with your project's title, or with Assignment 1 / 2\\ \smallskip}
% subtitle:
\large \explanation{\rm $\downarrow$ Keep only one of the following three  labels  / leave empty for assignments: $\downarrow$\\}

}
\author{Mattia Buzzoni,
Mirko Mornelli
\and
Riccardo Romeo
\\
Master's Degree in Artificial Intelligence, University of Bologna\\
\{ mattia.buzzoni, mirko.mornelli, riccardo.romeo \}@studio.unibo.it
}
\maketitle


\attention{DO NOT MODIFY THIS TEMPLATE - EXCEPT, OF COURSE FOR TITLE, SUBTITLE AND AUTHORS.\\ IN THE FINAL VERSION, IN THE \LaTeX\ SOURCE REMOVE THE \texttt{guidelines} OPTION FROM  \texttt{$\backslash$usepackage[guidelines]\{nlpreport\}}.
}

\begin{abstract}
%\begin{quote}
Comparative analysis of neural network architectures for tweet classification: evaluating LSTM and Transformer models across English and Spanish datasets, with a focus on preprocessing techniques including spell correction.

\explanation{
The abstract is very brief summary of your report. Try to keep it no longer than 15-20 lines at most. Write your objective, your approach, and your main observations (what are the findings that make this report worthwhile reading?)}

%\end{quote}
\end{abstract}

\attention{\textcolor{red}{NOTICE: THIS REPORT'S LENGTH MUST RESPECT THE FOLLOWING PAGE LIMITS: \begin{itemize}
    \item ASSIGNMENT: \textbf{2 PAGES} 
    \item NLP PROJECT OR PROJECT WORK: \textbf{8 PAGES}
    \item COMBINED NLP PROJECT + PW: \textbf{12 PAGES}
\end{itemize}  PLUS LINKS, REFERENCES AND APPENDICES.\\ 
THIS MEANS THAT YOU CANNOT FILL ALL SECTIONS TO MAXIMUM LENGTH. IT ALSO MEANS THAT, QUITE POSSIBLY, YOU WILL HAVE TO LEAVE OUT OF THE REPORT PART OF THE WORK YOU HAVE DONE OR OBSERVATIONS YOU HAVE. THIS IS NORMAL: THE REPORT SHOULD EMPHASIZE WHAT IS MOST SIGNIFICANT, NOTEWORTHY, AND REFER TO THE NOTEBOOK FOR ANYTHING ELSE.\\ 
FOR ANY OTHER ASPECT OF YOUR WORK THAT YOU WOULD LIKE TO EMPHASIZE BUT CANNOT EXPLAIN HERE FOR LACK OF SPACE, FEEL FREE TO ADD COMMENTS IN THE NOTEBOOK.\\ 
INTERESTING TEXT EXAMPLES THAT EXCEED THE MAXIMUM LENGTH OF THE REPORT CAN BE PLACED IN A DEDICATED APPENDIX AFTER THE REFERENCES.}}


\section{Introduction}
\label{sec:introduction}

This report investigates the performance of two architectural approaches—recurrent networks (LSTMs) and attention-based models (Transformers)—in classifying tweets across English and Spanish datasets. We implemented two preprocessing methods, including spell correction, and analyzed the impact of these techniques on model performance. Our findings indicate that Transformer models consistently outperformed LSTMs, achieving an average 10\% higher F1-Score and lower Binary Crossentropy values. Despite expectations, spell correction did not enhance performance. Notably, all models struggled with Spanish tweets, likely due to challenges in tokenization, slang, and cultural context. We propose modifications to the tokenization process to retain more information, such as using descriptive captions for emojis and maintaining hashtags within the text.

\attention{MAX 1 COLUMN FOR ASSIGNMENT REPORTS / 2 COLUMNS FOR PROJECT OR PW / 3 FOR COMBINED REPORTS.}

\explanation{
The Introduction is an executive summary, which you can think of as an extended abstract.  Start by writing a brief description of the problem you are tackling and why it is important. (Skip it if this is an assignment report).} 

\explanation{Then give a short overview of known/standard/possible approaches to that problems, if any, and what are their advantages/limitations.} 

\explanation{After that, discuss your approach, and motivate why you follow that approach. If you are drawing inspiration from an existing model, study, paper, textbook example, challenge, \dots, be sure to add all the necessary references~\cite{DBLP:journals/corr/abs-2204-02311,DBLP:conf/acl/LorenzoMN22,DBLP:conf/clef/AnticiBIIGR21,DBLP:conf/ijcai/NakovCHAEBPSM21,DBLP:conf/naacl/RottgerVHP22,DBLP:journals/toit/LippiT16}.\footnote{\href{https://en.wikipedia.org/wiki/The_Muppet_Show}{Add only what is relevant.}}}

\explanation{Next, give a brief summary of your experimental setup: how many experiments did you run on which dataset. Last, make a list of the main results or take-home lessons from your work.}

\attention{HERE AND EVERYWHERE ELSE: ALWAYS KEEP IN MIND THAT, CRUCIALLY, WHATEVER TEXT/CODE/FIGURES/IDEAS/... YOU TAKE FROM ELSEWHERE MUST BE CLEARLY IDENTIFIED AND PROPERLY REFERENCED IN THE REPORT.}

%\section{Background}
%\label{sec:background}
%\attention{MAX 2 COLUMNS (3 FOR COMBINED REPORTS). DO NOT INCLUDE SECTION IF NO BACKGROUND NECESSARY. OMIT SECTION IN ASSIGNMENT REPORTS.}

%\explanation{The Background section is where you briefly provide whatever background information on the domain or challenge you're addressing and/or on the techniques/approaches you're using, that (1) you think is necessary for the reader to understand your work and design choices, and (2) is not something that has been explained to you during the NLP course (to be clear: do NOT repeat explanations of things seen in class, we already know that stuff). If you adapt paragraphs from articles, books, online resources, etc: be sure to clarify which parts are yours and which ones aren't.}



\section{System description}
\label{sec:system}
We downloaded the dataset and encoded it as Pandas DataFrames, subsequently cleaning the tweets. A vocabulary was created based on the training dataset, and we utilized GloVe embeddings \cite{pennington-etal-2014-glove} with a dimensionality of 100. All out-of-vocabulary (OOV) terms were added to the embedding matrix, where they were assigned random embeddings.

Next, we implemented the BiLSTM model and the BiLSTM+LSTM architecture using Keras and TensorFlow. Additionally, we obtained several Transformer-based models from Hugging Face: RoBERTa, trained for hate speech detection; a fine-tuned BERTweet \cite{2020.emnlp-demos.2} specifically for sexism detection; and DeHateBert \cite{arXiv:2004.06465}, a fine-tuned model developed for hate detection in Spanish texts.


\attention{MAX 1 COLUMN FOR ASSIGNMENT REPORTS / 4 COLUMNS FOR PROJECT OR PW / 6 FOR COMBINED REPORTS.}

\explanation{
Describe the system or systems you have implemented (architectures, pipelines, etc), and used to run your experiments. If you reuse parts of code written by others, be sure to make very clear your original contribution in terms of
\begin{itemize}
    \item architecture: is the architecture your design or did you take it from somewhere else
    \item coding: which parts of code are original or heavily adapted? adapted from existing sources? taken from external sources with minimal adaptations?
\end{itemize}
It is a good idea to add figures to illustrate your pipeline and/or architecture(s)
(see Figure~\ref{fig:architecture})
%
%\begin{figure*}
%    \centering
%    \includegraphics[width=\textwidth]{img/architecture.pdf}
%    \caption{Model architecture}
%    \label{fig:architecture}
%\end{figure*}
}



%\section{Data}
%\label{sec:data}
%\attention{MAX 2 COLUMNS / 3 FOR COMBINED REPORTS. OMIT SECTION IN ASSIGNMENT REPORTS.}

%\explanation{Provide a brief description of your data including some statistics and pointers (references to articles/URLs) to be used to obtain the data. Describe any pre-processing work you did. Links to datasets must be placed later in Section~\ref{sec:links}.}


\section{Experimental setup and results}
\label{sec:results}

We manually tuned the hyperparameters for all the models utilized in our study. Specifically, we selected a learning rate of \(1 \times 10^{-3}\) and the AdamW optimizer for the LSTM models. We opted not to include dropout layers, as they did not yield better results. The models were trained for a maximum of 30 epochs. Notably, we allowed the initial embedding layer of the LSTM models to be trainable, as this improved performance.

In contrast, the Transformer models we downloaded already included specific dropout values, so we only adjusted the learning rate to \(2 \times 10^{-5}\). For all models, including the Transformer-based ones, we computed the macro-averaged F1 score and used Binary Crossentropy as the loss function.

We also implemented two types of preprocessing. The first followed the guidelines outlined in the assignment, while the second was tailored to the nature of the tweet-based dataset. In the second data-cleaning process, we performed spell correction on the tweets, based on the method described in Peter Norvig's blog \href{https://norvig.com/spell-correct.html}{here}, using TextBlob for implementation.

Finally, we decided to incorporate the Spanish dataset, training the LSTM models on Spanish tweets and comparing their performance with that of DeHateBert.


\attention{MAX 1 COLUMN FOR ASSIGNMENT REPORTS / 3 COLUMNS FOR PROJECT OR PW / 5 FOR COMBINED REPORTS.}

\explanation{
Describe how you set up your experiments: which architectures/configurations you used, which hyper-parameters and what methods used to set them, which optimizers, metrics, etc.
\\
Then, \textbf{use tables} to summarize your your findings (numerical results) in validation and test. If you don't have experience with tables in \LaTeX, you might want to use \href{https://www.tablesgenerator.com/}{\LaTeX table generator} to quickly create a table template.
}


\begin{table*}[!t]
\begin{tabular}{l|l|l|l|l|l|l|l|l}
\multicolumn{1}{c|}{\textbf{Architecture}} & \textbf{Params} & \textbf{Activation} & \textbf{LR} & \textbf{WD} & \textbf{Epochs} & \textbf{Loss Val.} & \textbf{F1 Val.} & \multicolumn{1}{c}{\textbf{F1 Test}} \\ \hline

Bi-LSTM + Dense		& 	3.2M	& sigmoid		& 1e-3	& 4e-3 	& 30 &  1.18  &	0.77 & 0.75 \\
Bi-LSTM + LSTM + Dense	& 		4.8M	& sigmoid		& 1e-3	& 4e-3	& 30 &	1.63 & 0.72 & 0.74\\
RoBERTa (hate detection)	&  125M	& gelu	& 2e-5	& 0.01		& 1	& 0.34 	& 0.88 & 0.85\\
BERTweet (sexism detection)	&  355M	& gelu+linear	& 2e-5	& 0.01	& 2	& 0.37  & 0.90 & 0.85\\

\end{tabular}
\caption{Results for every architecture on english dataset}
\end{table*}


\begin{table*}[!t]
\begin{tabular}{l|l|l|l|l|l|l|l|l}
\multicolumn{1}{c|}{\textbf{Architecture}} & \textbf{Params} & \textbf{Activation} & \textbf{LR} & \textbf{WD} & \textbf{Epochs} & \textbf{Loss Val.} & \textbf{F1 Val.} & \multicolumn{1}{c}{\textbf{F1 Test}} \\ \hline

Bi-LSTM + Dense		& 	3.2M	& sigmoid		& 1e-3	& 4e-3 	& 30 & 1.73 &	0.71 & 0.71 \\
Bi-LSTM + LSTM + Dense	& 		4.8M	& sigmoid		& 1e-3	& 4e-3	& 30 & 2.41 & 0.71 & 0.70\\
DeHateBert	&   167M	& tanh	& 2e-5	& 0.01	& 	2 & 0.45 &  0.80 & 0.79

\end{tabular}
\caption{Results for every architecture on spanish dataset}
\end{table*}




\section{Discussion}
\label{sec:discussion}
We observed a modest imbalance in the training dataset, with 60.4\% of the tweets classified as non-sexist. Additionally, the dataset is relatively small, containing only 2,870 elements, factors that may contribute to the low performance of the models. The implementation of spell correction did not yield better results, although it did reduce vocabulary size, leading to slightly faster training. However, the process was lengthy and ineffective for LSTM models.

All Transformer-based models outperformed the LSTMs in terms of Binary Crossentropy and F1-Score. We noted that the models struggled with classifying ironic tweets and those containing offensive language. Transformers demonstrated better performance in classifying some ironic tweets but struggled with extremely short and noisy tweets due to significant information loss in the tokenization process. For example, the tweet: \textit{"BOUNCEEYYYY BOOBIEEEESSSSSS \#hookup \#BOUNCEY \#boobs \#boobie \#tits"} is cleaned to: \textit{"bounceeyyyy boobieeeessssss"}, and tokenized to: \textit{"[UNK] [UNK]"}.

Both LSTM models and the Transformer-based model DeHateBert did not perform well on the Spanish dataset, suggesting that the Spanish language presents intrinsic complexities compared to English. To enhance model performance, we recommend using an automated hyperparameter finetuner and addressing dataset imbalance through data augmentation. Finally, preprocessing tweets to retain more information about emojis and hashtags is suggested.

\attention{MAX 1.5 COLUMNS FOR ASSIGNMENT REPORTS / 3 COLUMNS FOR PROJECT / 4 FOR COMBINED REPORTS. ADDITIONAL EXAMPLES COULD BE PLACED IN AN APPENDIX AFTER THE REFERENCES IF THEY DO NOT FIT HERE.}


\explanation{
Here you should make your analysis of the results you obtained in your experiments. Your discussion should be structured in two parts: 
\begin{itemize}
    \item discussion of quantitative results (based on the metrics you have identified earlier; compare with baselines);
    \item error analysis: show some examples of odd/wrong/unwanted  outputs; reason about why you are getting those results, elaborate on what could/should be changed in future developments of this work.
\end{itemize}
}



\section{Conclusion}
\label{sec:conclusion}
In this assignment, we evaluated the performance of recurrent networks (LSTMs) and attention-based models (Transformers) for tweet classification across English and Spanish datasets. Our results indicated that Transformer models consistently outperformed LSTMs, achieving an average of 10\% higher F1-Score and lower Binary Crossentropy values. Interestingly, the implementation of spell correction did not enhance performance as anticipated, and all models struggled more with Spanish tweets due to factors like tokenization challenges and cultural context.

We identified the tokenization process as a primary limitation, suggesting that modifications could improve results. Potential solutions include retaining more information during data cleaning by using descriptive captions for emojis, preserving hashtags, and introducing special tokens for user mentions.

\attention{MAX 1 COLUMN.}

\explanation{
In one or two paragraphs, recap your work and main results.
What did you observe? 
Did all go according to expectations? 
Was there anything surprising or worthwhile mentioning?
After that, discuss the main limitations of the solution you have implemented, and indicate promising directions for future improvement.
}




\section{Links to external resources}
\label{sec:links}
\attention{THIS SECTION IS OPTIONAL}
\explanation{
}


\begin{itemize}
    \item \href{https://textblob.readthedocs.io/en/dev/quickstart.html}{TextBlob}
    \item \href{https://huggingface.co/cardiffnlp/twitter-roberta-base-hate}{RoBERTa}
    \item \href{https://huggingface.co/tum-nlp/bertweet-sexism}{BERTweet}
    \item \href{https://huggingface.co/Hate-speech-CNERG/dehatebert-mono-spanish}{DeHateBert}
\end{itemize}


\attention{DO NOT INSERT CODE IN THIS REPORT}




\bibliography{nlpreport.bib}
\end{document}